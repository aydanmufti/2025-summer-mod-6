Homework reflection 1

As a preface, I have separate notebooks that I created for each week with the full code if needed for all of these answers. However for now I am just answering the questions with discussion of my process, visuals from my plots, and some snippets of code. I'm not sure if the notebooks were required for the submission. Please let me know if you need them. Thanks - AM

1. In Coding Quiz 1, you are asked to find the distance of the farthest match in a set.  Is this farthest match distance too far to be a meaningful match?  How can you decide this?

In the first quiz, I found that the farthest match distance is ~0.21. I believe that this farthest match distance is too far to be a meaningful match. This is because of several reasons:
a. The distance represents about 21% of the entire range of the Z variable. The Z variable spans from 0 to 1. This means that it is a substantial distance in the context of the dataset.
b. When using the radius approach in the quiz, we used a radius of 0.2 which might suggest that 0.2 is a reasonable threshold for meaningful matches. Sing 0.21 is over this (barely) we can say it shows that at least one match falls outside of what would be an acceptable distance.
c. At the end of the quiz, I visualized the nearest neighbor matching and radius matching. We can see that matches with larger distances connect points that appear visually distinct, suggesting poorer quality matching.
d. In the nearest neighbor matching, we saw that excessivly large distances can introduce bias because they force comparisons between units that are not fundamentally similar. This goes against the assumption that matches pairs should be similar on observed covariates.
We can further decide this by considering the distribution of the covariate (Z), examine how the effect estimate changes when excliding distanct matches, and apply domain knowledge about what a meaningful similarity would be in the context of the data.


2. In Coding Quiz 1, there are two approaches to matching: 
(A) Picking the best match X = 0 corresponding to each X = 1 using Z values.
(B) Using radius_neighbors to pick all matches X = 0 within a distance of 0.2 of each X = 1.

Invent your own type of matching similar to 1 and 2 (or look one up on the internet), which has a different way to pick the matches in X = 0.  Clearly explain the approach you invented or found.

For this quesiton I attempted Propensity Score Caliper Matching with Replacement
First we estimate the propensity scores. We do this by modeling the pobability of being in the treatment group (X = 1) based on Z. We do this by using logistic regression to estimate each observation's proabability of recieving treatment. These probablilties(propensity scores) become the bases for our matching. We transform the data by standardizing the matching variable and extend it naturally to multiple covariate.
We then apply caliper matching with replacemnt. This means that for each treatment observation (X = 1), we calculate distance to all control observation (X = 0) in terms of propensity scores. We then apply a caliper constraint by only considering controls within 0.1 std deviation of the propensity score distribution which in our case is 0.0168. We allow control observations to be used multiple times (which is done with replacemnt). For each treatment, we select all controls that fall within the caliper.
Lastly, we calculate the treatment effect. We do this by calculating the averate Y of the match controls for each matched treatment observation. We then compare the averate Y of treatment observations with an average of these control means. 

Here are the results of this process:
Propensity score standard deviation: 0.1684
Caliper width (0.1 std): 0.0168
Number of treatment observations matched: 29 out of 48
Average number of matches per treatment observation: 3.52
Minimum number of matches: 1
Maximum number of matches: 10
Number of unique control observations used: 39 out of 52
Controls used more than once: 30 (76.9%)

Treatment Effect Estimate:
Average Y for matched treatment observations: 1.0119
Average Y for matched control observations: 0.5088
Estimated treatment effect: 0.5031

From the results of this implementation we can make some key observations:
- 60.4% of treatment observations (29/48) found suitable matches
- Each matched treatment had an average of 3.52 control matches
- The number of matches varied widely (1-10 per treatment)
- The estimated treatment effect is 0.5031

There are several advantages we observed from performing this method.
- There is a balance between quality and quantity. This is because the caliper ensures matchi quality while allowing multiple matches where appropriate.
- There is an efficient use of data because the control observations can be reused when they provide good matches.
- Not all treatment observations need to be matches if there is no suitable control that exists.
- Using propensity scores and calibrating the caliper to the std deviation has theoretical justification. Plus we visualize mamtches via our histogram and scatter plots.

This method differs from the original two methods:
Comparison to Method A (Best 1:1 Match):
- Uses propensity scores instead of raw Z values
- Allows multiple control matches per treatment rather than just one
- Rejects treatment observations that have no good matches within the caliper
- Permits control observations to be matched multiple times

Comparison to Method B (Radius Matching):
- Uses a data-driven caliper (proportion of standard deviation) rather than fixed radius
- Matches based on propensity scores rather than raw Z values
- Scales the matching threshold according to the data's distribution

Homework reflection 2

1. Invent an example situation that would use fixed effects.

An example of a situation that would use fixed effects would be a situation that I am actually connected to and one that is strangly in some of the media I consume! My family works in education and is getting new technology at their school. There is also this tv show that involves different levels of tutoring and how that can help improve student's test scores that I am watching (Crash Course Romance). This makes me think of a situation that could use fixed effects:

We could look at a school district that wanted to evaluate the effectiveness of a new educational technology platform on student math scores. They could implement this program across 3 differnt types of schools and track the student performance scores over time.

This works with fixed effects as each school will have different characteristics based on resources that affect the performance of the students but not change overtime.
So school 1 can be located in a wealthy location with heavy parent involment and easy access to resources. School 2 can be an urban school with moderate resources and a larger diversity within students. School 3 will be a rural school with limited access to resources and smaller class sizes. 

To create our model:
MathScore = α_i + β*Time_t + ε_it

Where our MathScore is equal to the test score for school (i) at time (t). a_i is the fixed effect for school (i) as it captures each school baseline performance. β is the common effect of the technolgy program we implement over time and will be the same across all schools. Time_t is the months since the program was implemented.

The fixed effects (a_i) will account for time invariant school characteristics like location, demographics, and resources. The common slope (β) assumes the technology has the same rate of improvement across all schools. This will allow us to isolate the effect of our program while being able to monitor and control the differences between our chosen schools. 


2. Write a Python program that performs a bootstrap simulation to find the variance in the mean of the Pareto distribution when different samples are taken.  Explain what you had to do for this.  As you make the full sample size bigger (for the same distribution), what happens to the variance of the mean of the samples?  Does it stay about the same, get smaller, or get bigger?

For this - I had to generate original samples first. I used the samples 50, 100, 200, 500 1000, 2000, and 5000. I drew a random sample from the PAreto distribution and this became my "original dataset" to bootstrap from. For each original sample, I then performed 1000 bootstrap iterations where I randomly selected "n" observations from the original sample (where n is = to the original sample size) and found the mean of each bootstrap sample. So we ended up with 1000 bootstrap means. I then took all 1000 bootstrap means we calculated and calculated their variance. This was to understand how much the sample mean varies due to random sampling. I then repeated this for each sample size and compared how the cariance changed as the sample size increased. 

For my findings, I found that the variance gets smaller as the sample size increaes. To see this:
My Test (Pareto α=2.5):
n=50: Bootstrap variance = 0.009519
n=100: Bootstrap variance = 0.006925
n=500: Bootstrap variance = 0.007723
n=1000: Bootstrap variance = 0.000944
n=5000: Bootstrap variance = 0.000423

As we can see, as "n" increases, variance decreases dramatically. This is consistent with the Central Limit Theorem where the variance of sample mean = σ²/n and σ² is population variance and n is sample size.


Homework reflection 3

1. In the event study in Coding Quiz 3, how would we go about testing for a change in the second derivative as well?

To test for changes in the second derivative (curvature) at an event, we need to extend our regression model to include terms that capture acceleration changes.

The Standard Event Study model from Coding Quiz 3:
Y = α + β₁*time + β₂*post_event + β₃*(time-event_time)*post_event + ε
Note that
- β₂ captures level discontinuity (jump in Y)
- β₃ captures first derivative discontinuity (change in slope)

To extend the model for the second derivative, it would look like:
Y = α + β₁*time + β₂*time² + β₃*post_event + 
    β₄*(time-event_time)*post_event + β₅*(time-event_time)²*post_event + ε

The gamma variables change to represent:
- β₂ captures the baseline curvature (second derivative before event)
- β₅ captures the CHANGE in second derivative after the event

We can also write it as:
Y = α + β₁*time + β₂*time² + β₃*post_event + 
    β₄*time*post_event + β₅*time²*post_event + ε
where β₅ directly captures the change in curvature after the event


2. Create your own scenario that illustrates differences-in-differences. Describe the story behind the data and show whether there is a nonzero treatment effect.

My scenario is the Effect of Salary Cap on Competitive Balance in Professional Sports

The NHL is a professional sports leage with a hard salary cap whereas the MLB does not have one except for luxury tax. Let's say the NHL sets up a salary cap in 2020 and the MLB continues without thier cap. Let's measure the causal effect of salary caps on competitive balance in professional sports. 

Breakdown:
- Treatment Group: NHL Teams with implemented salary cap in 2020
- Control Group: MLB teams with no salary cap
- Time Period: Before (2015 - 2019) and After  (2020 - 2025) the cap implementation
- Outcome: Competitive balance measured by standard deviation of win percentages

We can use the differences-in-differences assuming that  without the salary cap, NHL and MLB would have had parallel trends in competitive balance. In sports, salary caps should reduce payroll disparities between teams, lower payroll disparities should lead to more balanced competition, and we can expect a reduced standard deviation of win percentages (more parity meaning a better competitve balance). 

My hypothesis: Salary cap implementation increases competitive balance

To test for nonzero treatment effect:
Research Question: Does the NHL salary cap have a nonzero effect on competitive balance?
Null Hypothesis (H₀): β₃ = 0 (no treatment effect)
Alternative Hypothesis (H₁): β₃ ≠ 0 (nonzero treatment effect)
- Where β₃ is the DID coefficient in: Y = α + β₁*NHL + β₂*Post + β₃*(NHL×Post) + ε

I will use multiple approaches to test this hypothesis:
1. Manual DID calculation with t-test
2. Regression-based DID with statistical inference
3. Bootstrap confidence intervals
4. Visual inspection of treatment effect

![alt text](week_3_reflection_q2.png)
Based on my analysis, I performed multiple statistical tests to determine whether the NHL salary cap had a nonzero treatment effect on competitive balance. I conducted a manual DID calculation with t-test and found a treatment effect of -0.0227 with a t-statistic of -4.534 and p-value of 0.000014, strongly rejecting the null hypothesis. I then performed a regression-based DID analysis using the model Y = α + β₁×NHL + β₂×Post + β₃×(NHL×Post) + ε and found the same coefficient (β₃ = -0.0227) with identical statistical significance (p < 0.0001). I calculated 95% and 99% confidence intervals using the regression standard errors and found that both intervals [-0.0326, -0.0128] and [-0.0358, -0.0096] exclude zero entirely. Additionally, I performed bootstrap analysis with 1,000 replications and found a bootstrap confidence interval of [-0.0314, -0.0129] that also excludes zero. All five testing approaches unanimously provide evidence of a statistically significant nonzero treatment effect, with the negative coefficient indicating that the salary cap successfully improved competitive balance by reducing the variation in team win percentages. For further review of my test code, please refer to week_3_quiz_reflection.ipynb.


Homework reflection 4

1. The Coding Quiz gives two options for instrumental variables.  For the second item (dividing the range of W into multiple ranges), explain how you did it, show your code, and discuss any issues you encountered.

To implement the W-Range Apprach, I devided the confounder W into multiple ranges and calculating IV estimates within each range.

Step 1 - Create The Range:
/# Create narrow ranges of W
W_min, W_max = W.min(), W.max()
n_ranges = 10  # Number of ranges created
range_width = (W_max - W_min) / n_ranges

print(f"W ranges from {W_min:.3f} to {W_max:.3f}")
print(f"Creating {n_ranges} ranges, each of width {range_width:.3f}")

The result from this was that:
- W ranged from -3.303 to 4.783
- Created 10 equal-width ranges, each of width 0.809

Step 2 - Estimate IV Within Each Range
iv_estimates_by_range = []
range_centers = []

for i in range(n_ranges):
    # Initializing range boundaries
    w_lower = W_min + i * range_width
    w_upper = W_min + (i + 1) * range_width
    range_center = (w_lower + w_upper) / 2
    
    # Select observations in this range
    in_range = (W >= w_lower) & (W < w_upper)
    if i == n_ranges - 1:  # Include the max value in the last range
        in_range = (W >= w_lower) & (W <= w_upper)
    
    n_obs_in_range = np.sum(in_range)
    
    # Skip ranges with insufficient data
    if n_obs_in_range < 10:
        continue
    
    # Extract data for this range
    Z_range = Z[in_range]
    X_range = X[in_range]
    Y_range = Y[in_range]
    
    # Check if we have both Z=0 and Z=1 in this range
    has_Z0 = np.any(Z_range == 0)
    has_Z1 = np.any(Z_range == 1)
    
    if not (has_Z0 and has_Z1):
        continue
    
    # Calculate IV estimate within this range (Wald estimator)
    Y_mean_Z1_range = Y_range[Z_range == 1].mean()
    Y_mean_Z0_range = Y_range[Z_range == 0].mean()
    X_mean_Z1_range = X_range[Z_range == 1].mean()
    X_mean_Z0_range = X_range[Z_range == 0].mean()
    
    delta_Y_range = Y_mean_Z1_range - Y_mean_Z0_range
    delta_X_range = X_mean_Z1_range - X_mean_Z0_range
    
    if abs(delta_X_range) > 1e-10:
        iv_estimate_range = delta_Y_range / delta_X_range
        iv_estimates_by_range.append(iv_estimate_range)
        range_centers.append(range_center)


I then collected the results:

/# Average the IV estimates across ranges
if iv_estimates_by_range:
    iv_estimate_approach2 = np.mean(iv_estimates_by_range)
    iv_std_approach2 = np.std(iv_estimates_by_range)

    print(f"\nIV Estimate (Approach 2 - Average across ranges): {iv_estimate_approach2:.4f}")
    print(f"Standard deviation across ranges: {iv_std_approach2:.4f}")
    
    # Plot IV estimates by range
    plt.figure(figsize=(10, 6))
    plt.scatter(range_centers, iv_estimates_by_range, alpha=0.7, s=50)
    plt.axhline(y=iv_estimate_approach2, color='red', linestyle='--', 
                label=f'Average IV Estimate = {iv_estimate_approach2:.4f}')
    if 'iv_estimate_approach1' in locals():
        plt.axhline(y=iv_estimate_approach1, color='blue', linestyle='--', 
                    label=f'Approach 1 Estimate = {iv_estimate_approach1:.4f}')
    plt.xlabel('W Range Center')
    plt.ylabel('IV Estimate')
    plt.title('IV Estimates by W Range (Approach 2)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
else:
    print("Could not calculate IV estimates for any ranges")

For the results I got:
W ranges from -3.303 to 4.783
Creating 10 ranges, each of width 0.809
Range [-3.30, -2.49]: n=36, IV estimate=1.1462
Range [-2.49, -1.69]: n=158, IV estimate=1.8387
Range [-1.69, -0.88]: n=773, IV estimate=1.4396
Range [-0.88, -0.07]: n=1452, IV estimate=1.5340
Range [-0.07, 0.74]: n=1474, IV estimate=1.5724
Range [0.74, 1.55]: n=789, IV estimate=1.5084
Range [1.55, 2.36]: n=272, IV estimate=1.3114
Range [2.36, 3.17]: n=38, IV estimate=1.5150

IV Estimate (Approach 2 - Average across ranges): 1.4832
Standard deviation across ranges: 0.1883
![alt text](ivestimatesbywrange.png)


For the final results the overall IV estimate was 1.4832. The standard deviation across ranges was 0.1883. To compare with my initial approach of calculating the simple differences in means, approach 1 calculated 1.5619.

Issues:
Some issues that I found involved:
a. Sparse data in some of the extreme ranges
    - Some W ranges, mainly at the extremes, above had very few observations. A solution for this would be to skip the ranges with less than 10 observations to avoid unreliable estimates. So for me, two ranges were excluded from analysis to improve estimate reliability.
b. Missing some treatment groups
    - Some of my ranges missed the Z=0 or Z=1 observations. We can then only analyze rnages with both treatment and control groups. This is because we cannot calculate IV estimate without both groups for comparison.
c. Instrument weakness in some of the ranges
    - Some of the ranges had delta X = 0, which means the instument doesn't affect the treatment. To mitigate this I skiped ranges where the absolute value of delta X is less than 1e-10. We did this because these ranges do not contribute to causal identification.



2. Plot the college outcome (Y) vs. the test score (X) in a small range of test scores around 80. On the plot, compare it with the Y probability predicted by logistic regression. The ground truth Y value is 0 or 1; don't just plot 0 or 1 - that will make it unreadable.  Find some way to make it look better than that.

![alt text](DatasetA_CollegeOutcomeVSTestScore.png)

![alt text](DatasetB_CollegeOutcomeVSTestScore.png)

Analysis of Dataset A
------------------------------------------------------------
Total observations in dataset: 100000
Observations in window [60, 100]: 97772
   X range in window: [60.0, 100.0]
   Y range in window: [0.0, 1.0]
   Unique Y values: [0 1]
   Binary outcome (0/1): True
   Observations before cutoff: 47698
   Observations after cutoff: 50074

   LOGISTIC REGRESSION RESULTS:
   - Overall coefficient: 0.056408
   - Overall intercept: -4.724235
   - Predicted probability at cutoff (80): 0.4473
   - Probability just before cutoff: 0.3051
   - Probability just after cutoff: 0.6008
   - Discontinuity at cutoff: 0.2957
   - Slope before cutoff: 0.001231
   - Slope after cutoff: 0.000672
   - Slope change: -0.000560
   - Large discontinuity - strong treatment effect

Analysis of Dataset B
------------------------------------------------------------
Total observations in dataset: 100000
Observations in window [60, 100]: 97716
   X range in window: [60.0, 100.0]
   Y range in window: [0.0, 1.0]
   Unique Y values: [0 1]
   Binary outcome (0/1): True
   Observations before cutoff: 47784
   Observations after cutoff: 49932

   LOGISTIC REGRESSION RESULTS:
   - Overall coefficient: 0.115625
   - Overall intercept: -7.706836
   - Predicted probability at cutoff (80): 0.8239
   - Probability just before cutoff: 0.6998
   - Probability just after cutoff: 0.8853
   - Discontinuity at cutoff: 0.1855
   - Slope before cutoff: 0.044387
   - Slope after cutoff: 0.106180
   - Slope change: 0.061793
   - Large discontinuity - strong treatment effect

I used 5 solutions to analyze and plot this data to make it readable.
1. Adding Jitter To Raw Data Scatter Plot
   - Added small random noise (±0.05) to Y-axis to spread overlapping points
   - Used color coding (red/blue colormap) to distinguish actual 0s from 1s
   - Added black edge colors for better point definition
   - This allowed us to see individual observations while preserving binary nature

2. Plotting Binned Averages:
   - Grouped observations into bins along X-axis
   - Calculated mean Y within each bin (proportion who got into college)
   - Sized points by number of observations in bin
   - Overlaid smooth logistic regression curve
   - With this plot we can see that it is cleaner than individual 0/1 points, shows clear trends

3. Plotting Moving Averages With Confidence Bands: 
   - Calculated local averages using sliding window
   - Added 95% confidence intervals (±1.96 * standard error)
   - Smooths out noise while preserving local trends
   - This shows uncertainty and local probability trends

4. Separating Logistic Fits
   - Fit different logistic models before and after cutoff
   - Allows for discontinuities at the cutoff point
   - Shows potential treatment effects clearly
   - This plot reveals regression discontinuity as jump in probability

5. Logistic Regression Overlay:
   - Fits smooth probability curve through all data
   - Transforms binary outcome to interpretable probabilities
   - Red curve shows predicted probability of college admission
   - This allows for a natural interpretation for binary outcomes