Homework reflection 1

1. In Coding Quiz 1, you are asked to find the distance of the farthest match in a set.  Is this farthest match distance too far to be a meaningful match?  How can you decide this?

In the first quiz, I found that the farthest match distance is ~0.21. I believe that this farthest match distance is too far to be a meaningful match. This is because of several reasons:
a. The distance represents about 21% of the entire range of the Z variable. The Z variable spans from 0 to 1. This means that it is a substantial distance in the context of the dataset.
b. When using the radius approach in the quiz, we used a radius of 0.2 which might suggest that 0.2 is a reasonable threshold for meaningful matches. Sing 0.21 is over this (barely) we can say it shows that at least one match falls outside of what would be an acceptable distance.
c. At the end of the quiz, I visualized the nearest neighbor matching and radius matching. We can see that matches with larger distances connect points that appear visually distinct, suggesting poorer quality matching.
d. In the nearest neighbor matching, we saw that excessivly large distances can introduce vias because they force comparisons between units that are not fundamentally similar. This goes against the assumption that matches pairs should be similar on observed covariates.
We can further decide this by considering the distribution of the covariate (Z), examine how the effect estimate changes when excliding distanct matches, and apply domain knowledge about what a meaningful similarity would be in the context of the data.


2. In Coding Quiz 1, there are two approaches to matching: 
(A) Picking the best match X = 0 corresponding to each X = 1 using Z values.
(B) Using radius_neighbors to pick all matches X = 0 within a distance of 0.2 of each X = 1.

Invent your own type of matching similar to 1 and 2 (or look one up on the internet), which has a different way to pick the matches in X = 0.  Clearly explain the approach you invented or found.

For this quesiton I attempted Propensity Score Caliper Matching with Replacement
First we estimate the propensity scores. We do this by modeling the pobability of being in the treatment group (X = 1) based on Z. We do this by using logistic regression to estimate each observation's proabability of recieving treatment. These probablilties(propensity scores) become the bases for our matching. We transform the data by standardizing the matching variable and extend it naturally to multiple covariate.
We then apply caliper matching with replacemnt. This means that for each treatment observation (X = 1), we calculate distance to all control observation (X = 0) in terms of propensity scores. We then apply a caliper constraint by only considering controls within 0.1 std deviation of the propensity score distribution which in our case is 0.0168. We allow control observations to be used multiple times (which is done with replacemnt). For each treatment, we select all controls that fall within the caliper.
Lastly, we calculate the treatment effect. We do this by calculating the averate Y of the match controls for each matched treatment observation. We then compare the averate Y of treatment observations with an average of these control means. 

Here are the results of this process:
Propensity score standard deviation: 0.1684
Caliper width (0.1 std): 0.0168
Number of treatment observations matched: 29 out of 48
Average number of matches per treatment observation: 3.52
Minimum number of matches: 1
Maximum number of matches: 10
Number of unique control observations used: 39 out of 52
Controls used more than once: 30 (76.9%)

Treatment Effect Estimate:
Average Y for matched treatment observations: 1.0119
Average Y for matched control observations: 0.5088
Estimated treatment effect: 0.5031

From the results of this implementation we can make some key observations:
- 60.4% of treatment observations (29/48) found suitable matches
- Each matched treatment had an average of 3.52 control matches
- The number of matches varied widely (1-10 per treatment)
- The estimated treatment effect is 0.5031

There are several advantages we observed from performing this method.
- There is a balance between quality and quantity. This is because the caliper ensures matchi quality while allowing multiple matches where appropriate.
- There is an efficient use of data because the control observations can be reused when they provide good matches.
- Not all treatment observations need to be matches if there is no suitable control that exists.
- Using propensity scores and calibrating the caliper to the std deviation has theoretical justification. Plus we visualize mamtches via our histogram and scatter plots.

This method differs from the original two methods:
Versus Method A (Best 1:1 Match):
- Uses propensity scores instead of raw Z values
- Allows multiple control matches per treatment rather than just one
- Rejects treatment observations that have no good matches within the caliper
- Permits control observations to be matched multiple times


Versus Method B (Radius Matching):
- Uses a data-driven caliper (proportion of standard deviation) rather than fixed radius
- Matches based on propensity scores rather than raw Z values
- Scales the matching threshold according to the data's distribution

Homework reflection 2

1. Invent an example situation that would use fixed effects.

2. Write a Python program that performs a bootstrap simulation to find the variance in the mean of the Pareto distribution when different samples are taken.  Explain what you had to do for this.  As you make the full sample size bigger (for the same distribution), what happens to the variance of the mean of the samples?  Does it stay about the same, get smaller, or get bigger?

Homework reflection 3

1. In the event study in Coding Quiz 3, how would we go about testing for a change in the second derivative as well?
To test for changes in the second derivative (curvature) at an event, we need to extend our regression model to include terms that capture acceleration changes.

The Standard Event Study model from Coding Quiz 3:
Y = α + β₁*time + β₂*post_event + β₃*(time-event_time)*post_event + ε
Note that
- β₂ captures level discontinuity (jump in Y)
- β₃ captures first derivative discontinuity (change in slope)

To extend the model for the second derivative, it would look like:
Y = α + β₁*time + β₂*time² + β₃*post_event + 
    β₄*(time-event_time)*post_event + β₅*(time-event_time)²*post_event + ε

The gamma variables change to represent:
- β₂ captures the baseline curvature (second derivative before event)
- β₅ captures the CHANGE in second derivative after the event

We can also write it as:
Y = α + β₁*time + β₂*time² + β₃*post_event + 
    β₄*time*post_event + β₅*time²*post_event + ε
where β₅ directly captures the change in curvature after the event


2. Create your own scenario that illustrates differences-in-differences. Describe the story behind the data and show whether there is a nonzero treatment effect.

My scenario is the Effect of Salary Cap on Competitive Balance in Professional Sports

The NHL is a professional sports leage with a hard salary cap whereas the MLB does not have one except for luxury tax. Let's say the NHL sets up a salary cap in 2020 and the MLB continues without thier cap. Let's measure the causal effect of salary caps on competitive balance in professional sports. 

Breakdown:
- Treatment Group: NHL Teams with implemented salary cap in 2020
- Control Group: MLB teams with no salary cap
- Time Period: Before (2015 - 2019) and After  (2020 - 2025) the cap implementation
- Outcome: Competitive balance measured by standard deviation of win percentages

We can use the differences-in-differences assuming that  without the salary cap, NHL and MLB would have had parallel trends in competitive balance. In sports, salary caps should reduce payroll disparities between teams, lower payroll disparities should lead to more balanced competition, and we can expect a reduced standard deviation of win percentages (more parity meaning a better competitve balance). 



Hypothesis: Salary cap implementation increases competitive balance


Homework reflection 4

1. The Coding Quiz gives two options for instrumental variables.  For the second item (dividing the range of W into multiple ranges), explain how you did it, show your code, and discuss any issues you encountered.

To implement the W-Range Apprach, I devided the confounder W into multiple ranges and calculating IV estimates within each range.

Step 1 - I created the range:
/# Create narrow ranges of W
W_min, W_max = W.min(), W.max()
n_ranges = 10  # Number of ranges to create
range_width = (W_max - W_min) / n_ranges

print(f"W ranges from {W_min:.3f} to {W_max:.3f}")
print(f"Creating {n_ranges} ranges, each of width {range_width:.3f}")

The result from this was that:
- W ranged from -3.303 to 4.783
- Created 10 equal-width ranges, each of width 0.809

Step 2 - Estimate IV Within Each Range
iv_estimates_by_range = []
range_centers = []

for i in range(n_ranges):
    # Define range boundaries
    w_lower = W_min + i * range_width
    w_upper = W_min + (i + 1) * range_width
    range_center = (w_lower + w_upper) / 2
    
    # Select observations in this range
    in_range = (W >= w_lower) & (W < w_upper)
    if i == n_ranges - 1:  # Include the maximum value in the last range
        in_range = (W >= w_lower) & (W <= w_upper)
    
    n_obs_in_range = np.sum(in_range)
    
    # Skip ranges with insufficient data
    if n_obs_in_range < 10:
        continue
    
    # Extract data for this range
    Z_range = Z[in_range]
    X_range = X[in_range]
    Y_range = Y[in_range]
    
    # Check if we have both Z=0 and Z=1 in this range
    has_Z0 = np.any(Z_range == 0)
    has_Z1 = np.any(Z_range == 1)
    
    if not (has_Z0 and has_Z1):
        continue
    
    # Calculate IV estimate within this range (Wald estimator)
    Y_mean_Z1_range = Y_range[Z_range == 1].mean()
    Y_mean_Z0_range = Y_range[Z_range == 0].mean()
    X_mean_Z1_range = X_range[Z_range == 1].mean()
    X_mean_Z0_range = X_range[Z_range == 0].mean()
    
    delta_Y_range = Y_mean_Z1_range - Y_mean_Z0_range
    delta_X_range = X_mean_Z1_range - X_mean_Z0_range
    
    if abs(delta_X_range) > 1e-10:
        iv_estimate_range = delta_Y_range / delta_X_range
        iv_estimates_by_range.append(iv_estimate_range)
        range_centers.append(range_center)


I then collected the results:

/# Average the IV estimates across ranges
if iv_estimates_by_range:
    iv_estimate_approach2 = np.mean(iv_estimates_by_range)
    iv_std_approach2 = np.std(iv_estimates_by_range)

    print(f"\nIV Estimate (Approach 2 - Average across ranges): {iv_estimate_approach2:.4f}")
    print(f"Standard deviation across ranges: {iv_std_approach2:.4f}")
    
    # Plot IV estimates by range
    plt.figure(figsize=(10, 6))
    plt.scatter(range_centers, iv_estimates_by_range, alpha=0.7, s=50)
    plt.axhline(y=iv_estimate_approach2, color='red', linestyle='--', 
                label=f'Average IV Estimate = {iv_estimate_approach2:.4f}')
    if 'iv_estimate_approach1' in locals():
        plt.axhline(y=iv_estimate_approach1, color='blue', linestyle='--', 
                    label=f'Approach 1 Estimate = {iv_estimate_approach1:.4f}')
    plt.xlabel('W Range Center')
    plt.ylabel('IV Estimate')
    plt.title('IV Estimates by W Range (Approach 2)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
else:
    print("Could not calculate IV estimates for any ranges")

For the results I got:
W ranges from -3.303 to 4.783
Creating 10 ranges, each of width 0.809
Range [-3.30, -2.49]: n=36, IV estimate=1.1462
Range [-2.49, -1.69]: n=158, IV estimate=1.8387
Range [-1.69, -0.88]: n=773, IV estimate=1.4396
Range [-0.88, -0.07]: n=1452, IV estimate=1.5340
Range [-0.07, 0.74]: n=1474, IV estimate=1.5724
Range [0.74, 1.55]: n=789, IV estimate=1.5084
Range [1.55, 2.36]: n=272, IV estimate=1.3114
Range [2.36, 3.17]: n=38, IV estimate=1.5150

IV Estimate (Approach 2 - Average across ranges): 1.4832
Standard deviation across ranges: 0.1883
![alt text](image.png)


For the final results the overall IV estimate was 1.4832. The standard deviation across ranges was 0.1883. To compare with my initial approach of calculating the simple differences in means, approach 1 calculated 1.5619.

Issues:
Some issues that I found involved:
a. Sparse data in some of the extreme ranges
    - Some W ranges mainly at the extremes above had very few observations. A solution for this would


2. Plot the college outcome (Y) vs. the test score (X) in a small range of test scores around 80. On the plot, compare it with the Y probability predicted by logistic regression. The ground truth Y value is 0 or 1; don't just plot 0 or 1 - that will make it unreadable.  Find some way to make it look better than that.
