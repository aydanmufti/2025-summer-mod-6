Homework Reflection 5

1. Draw a diagram for the following negative feedback loop:

Sweating causes body temperature to decrease.  High body temperature causes sweating.

A negative feedback loop means that one thing increases another while the second thing decreases the first.

Remember that we are using directed acyclic graphs where two things cannot directly cause each other.

Answer: 
To represent the negative feedback loop between body temperature and sweating in a DAG, we use time subscripts to break the cycle:
Temp(t) → Sweat(t+1): High body temperature at time t causes sweating at time t+1
Sweat(t) → Temp(t+1): Sweating at time t decreases body temperature at time t+1
This maintains the feedback relationship while creating a valid DAG with no cycles. The diagram shows how each variable at one time point affects the other variable at the next time point, preserving the causal logic while avoiding the prohibited cycle.
![HWReflections5-8.png](HWReflections5-8.png)

2. Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.
An example of a positive feedback loop would be: Social Media Engagement
This works by:
More posts → More followers (quality content attracts audience) -> More followers → More motivation to post (larger audience provides encouragement)
Both variables reinforce each other upward, leading to exponential growth



3. Draw a diagram for the following situation:

Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.
Bears eat deer, decreasing their population.
Deer eat flowers, decreasing their population.

Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.

Identify a backdoor path with one or more confounders for the relationship between deer and flowers.
![alt text](HWReflections5-8.png)
Causal relationships:
Lightning → Deer/Bears (negative): Storms frighten animals away
Lightning → Flowers (positive): Storms promote flower growth
Bears → Deer (negative): Bears eat deer
Deer → Flowers (negative): Deer eat flowers

Dataset that simulates this situation:
# Lightning storms (Poisson distribution, avg 3 per year)
lightning = np.random.poisson(3, n)

# Lightning effects with noise
deer_pop = max(1, baseline - lightning*5 + noise)     # Lightning scares deer
bear_pop = max(1, baseline - lightning*2 + noise)     # Lightning scares bears  
flower_pop = max(1, baseline + lightning*10 + noise)  # Lightning helps flowers

# Predation effects with noise
deer_pop = max(1, deer_pop - bear_pop*0.3 + noise)    # Bears eat deer
flower_pop = max(1, flower_pop - deer_pop*0.8 + noise) # Deer eat flowers



Backdoor path identification:
For the relationship Deer → Flowers, there is a confounding backdoor path:
Deer ← Lightning → Flowers

Lightning is a confounder because it affects both deer (negatively) and flowers (positively), creating a spurious association. To estimate the true causal effect of deer on flowers, we must control for lightning storms.

4. Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?
![alt text](HWReflections5-8.png)
Variables:
Treatment (X): Education Level
Outcome (Y): Annual Income
Confounder: Family Wealth (affects both education and income)
Collider: Job Satisfaction (affected by both education and income)
Mediator: Professional Networking

Key relationships:
Confounder - Family Wealth:
Family Wealth → Education (wealthy families afford better education)
Family Wealth → Income (inheritance, connections, opportunities)
Creates backdoor path: Education ← Family Wealth → Income

Collider - Job Satisfaction:
Education → Job Satisfaction (education brings fulfillment)
Income → Job Satisfaction (money reduces financial stress)
Controlling for job satisfaction would introduce collider bias
Mediator - Professional Networking:
Education → Networking → Income (causal pathway)



Homework Reflection 6

1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)
Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.
(Please explain / justify this answer, or give a different one if you can think of one.)

The fundamental problem with computing the Marginal Treatment Effect by simply taking the maximum difference is that this approach is highly susceptible to random noise and sampling variability, leading to systematically inflated estimates. This occurs because when we select the most extreme observation from a distribution of treatment effects. We essentially cherry-pick an outlier that likely contains both the true treatment effect and a significant amount of random error. The statistical problem manifests in several ways: selection bias (systematically choosing the most extreme case), noise amplification (extreme values typically contain more measurement error), poor generalizability (one extreme case may not represent the true population MTE), and high variance across different samples. This is analogous to the multiple testing problem in statistics, where examining many comparisons produces extreme results purely by chance, making the maximum-based MTE an unreliable estimator of the true marginal treatment effect.

2. Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.
Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.
(Either code this answer or choose a different one.)

The most effective solution to remedy the noise amplification problem is to use a percentile-based approach instead of the strict maximum, specifically employing the 90th percentile as a robust estimator of the Marginal Treatment Effect (like suggested). We can simply start by replacing the problematic maximum calculation:

mte = np.max(treatment_effects)  # Returns 2.1725 (inflated by noise)
robust_mte = np.percentile(treatment_effects, 90)  # Returns 1.9280

This approach yields an MTE of 1.9280, which is 0.2445 units lower than the problematic maximum-based estimate of 2.1725, suggesting that nearly 11% of the original estimate was likely due to noise. The 90th percentile method works effectively because it still captures individuals with high treatment effects while systematically excluding the top 10% most extreme cases that are most likely to be inflated by random variation.
Additionally, several alternative robust solutions can be implemented with simple code modifications. The average of the top K effects approach reduces the influence of any single extreme observation:

# Average of top 5 effects (returns 2.1454)
sorted_effects = sorted(treatment_effects, reverse=True)
top_k_average = np.mean(sorted_effects[:5])

A trimmed maximum approach removes extreme outliers before taking the maximum:
# Trimmed maximum (remove top/bottom 5%, returns ~2.0150)
def trimmed_max(effects, trim_percent=5):
    lower_bound = np.percentile(effects, trim_percent)
    upper_bound = np.percentile(effects, 100 - trim_percent)
    trimmed_effects = [x for x in effects if lower_bound <= x <= upper_bound]
    return np.max(trimmed_effects)
trimmed_mte = trimmed_max(treatment_effects, 5)

For a more comprehensive approach, bootstrap analysis can quantify the uncertainty in MTE estimates:
# Bootstrap confidence intervals
def bootstrap_mte(treated_data, untreated_data, n_bootstrap=500):
    bootstrap_mtes = []
    for _ in range(n_bootstrap):
        # Resample both groups
        treated_sample = treated_data.sample(n=len(treated_data), replace=True)
        untreated_sample = untreated_data.sample(n=len(untreated_data), replace=True)
        
        # Calculate MTE for bootstrap sample
        # [matching code here...]
        bootstrap_mtes.append(sample_mte)
    
    return np.mean(bootstrap_mtes), np.std(bootstrap_mtes)

bootstrap_mean, bootstrap_std = bootstrap_mte(treated, untreated)
# Returns: mean=2.1614, std=0.0146

The recommended implementation combines simplicity with statistical robustness:
def robust_marginal_treatment_effect(treated_data, untreated_data, percentile=90):
    # Fit nearest neighbors on treated group
    nn = NearestNeighbors(n_neighbors=1, metric='euclidean')
    nn.fit(treated_data[['Z']].values)
    
    # Calculate treatment effects for all untreated observations
    treatment_effects = []
    for idx, untreated_row in untreated_data.iterrows():
        distances, indices = nn.kneighbors([[untreated_row['Z']]])
        nearest_treated = treated_data.iloc[indices[0][0]]
        effect = nearest_treated['Y'] - untreated_row['Y']
        treatment_effects.append(effect)
    
    # Use percentile instead of maximum to reduce noise
    return np.percentile(treatment_effects, percentile)

# Usage:
robust_mte = robust_marginal_treatment_effect(treated, untreated, percentile=90)
# Returns 1.9280 vs 2.1725 for maximum-based approach

This percentile-based approach provides the optimal balance between capturing meaningful high treatment effects and avoiding the statistical pitfalls of extreme value selection, resulting in a more reliable and generalizable estimate of the true Marginal Treatment Effect with minimal code modification required.

Homework Reflection 7

1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between $$X$$ and $$Y$$ is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.

I created a linear regression model to demonstrate how omitting a confounder leads to biased estimates of the relationship between a treatment variable (X) and outcome variable (Y). The scenario that I'm modeling is the effect of study hours per week on test scores, with intelligence serving as an unobserved confounder.

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Set random seed for reproducibility
np.random.seed(42)
n = 1000

# True model coefficients
beta_0 = 50    # Baseline test score
beta_x = 2     # True effect of study hours on test score
beta_c = 15    # Effect of intelligence on test score
gamma_0 = 5    # Baseline study hours
gamma_c = 3    # Effect of intelligence on study hours

# Generate the confounder (intelligence)
C = np.random.normal(0, 1, n)

# Generate X (study hours) - influenced by intelligence
X = gamma_0 + gamma_c * C + np.random.normal(0, 1, n)

# Generate Y (test score) - influenced by both study hours and intelligence
Y = beta_0 + beta_x * X + beta_c * C + np.random.normal(0, 2, n)

# Fit the correct model (includes confounder)
correct_model = LinearRegression()
correct_model.fit(np.column_stack([X, C]), Y)

# Fit the incorrect model (omits confounder)
incorrect_model = LinearRegression()
incorrect_model.fit(X.reshape(-1, 1), Y)

print(f"True effect of study hours: {beta_x}")
print(f"Estimated effect (correct model): {correct_model.coef_[0]:.4f}")
print(f"Estimated effect (incorrect model): {incorrect_model.coef_[0]:.4f}")
print(f"Bias: {incorrect_model.coef_[0] - beta_x:.4f}")

The analysis reveals that the true correlation between study hours and test scores is severely overestimated when the intelligence confounder is omitted from the model. Specifically, the true effect of study hours on test scores is 2 points per additional hour studied. When intelligence is properly controlled for in the regression, the estimated effect is 1.98 points (very close to the true value). However, when intelligence is omitted from the model, the estimated effect jumps dramatically to 6.54 points per hour. This is an overestimation of 227%.
This substantial overestimation occurs because intelligence acts as a positive confounder that influences both the treatment (study hours) and the outcome (test scores) in the same direction. Students with higher intelligence tend to study more hours (γc = 3) and also tend to achieve higher test scores independently of their study time (βc = 15). When the regression model omits intelligence, it incorrectly attributes the portion of test score variation that is actually due to intelligence differences to the study hours variable instead.
The mathematical foundation for this bias follows the omitted variable bias formula: Bias = (βc × γc) / Var(X), where βc is the effect of the confounder on the outcome, γc is the effect of the confounder on the treatment, and Var(X) is the variance of the treatment variable. In this case, the theoretical bias is (15 × 3) / 9.38 = 4.80, which closely matches the observed bias of 4.54. Since both βc and γc are positive, their product creates positive bias, leading to overestimation of the true relationship. This example illustrates a fundamental principle in causal inference: when a confounder has effects in the same direction on both the treatment and outcome variables, omitting it from the model will lead to overestimation of the treatment effect. Conversely, if the confounder had opposite-direction effects (positive on one variable, negative on the other), we would observe underestimation. The magnitude of bias depends on the strength of the confounder's relationships with both variables and the variance of the treatment variable. This demonstrates why proper identification and control of confounders is crucial for obtaining unbiased estimates of causal relationships in observational data.

2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.

W = [noise]
X = [noise]
Y = 2 * X + [noise]

And compute the p-value of a coefficient - in this case, the coefficient of W.  
(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)
If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)
Run the analysis 1000 times and report the best (smallest) p-value.  
If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?

I conducted a simulation to demonstrate the multiple testing problem by performing 1000 independent linear regression analyses where one coefficient is known to be exactly zero. The model structure had variable W having no effect on the outcome Y, allowing us to identify false positive results when p-values fall below 0.05.
# imports
import numpy as np
from sklearn.linear_model import LinearRegression
from scipy import stats

def run_single_regression():
    n = 100  # Sample size
    
    # Generate data where W has NO effect on Y
    W = np.random.normal(0, 1, n)  # W is just noise
    X = np.random.normal(0, 1, n)  # X is just noise
    Y = 2 * X + np.random.normal(0, 1, n)  # Y depends only on X, not W
    
    # Fit regression: Y = β₀ + β₁W + β₂X + ε
    design_matrix = np.column_stack([W, X])
    model = LinearRegression()
    model.fit(design_matrix, Y)
    
    # Calculate p-value for W coefficient using t-test
    y_pred = model.predict(design_matrix)
    residuals = Y - y_pred
    df_resid = n - 3  # degrees of freedom
    mse = np.sum(residuals**2) / df_resid
    
    full_design = np.column_stack([np.ones(n), design_matrix])
    cov_matrix = mse * np.linalg.inv(full_design.T @ full_design)
    std_errors = np.sqrt(np.diag(cov_matrix))
    
    w_coefficient = model.coef_[0]  # Coefficient of W
    w_std_error = std_errors[1]    # Standard error of W coefficient
    t_stat = w_coefficient / w_std_error
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df_resid))  # Two-tailed test
    
    return p_value, w_coefficient

# Running 1000 independent analyses
n_simulations = 1000
p_values = []
coefficients = []

for i in range(n_simulations):
    p_val, coef = run_single_regression()
    p_values.append(p_val)
    coefficients.append(coef)

p_values = np.array(p_values)
min_p_value = np.min(p_values)
significant_count = np.sum(p_values < 0.05)
false_positive_rate = significant_count / n_simulations

print(f"Smallest p-value found: {min_p_value:.6f}")
print(f"Number of p-values < 0.05: {significant_count}")
print(f"False positive rate: {false_positive_rate:.3f}")

The simulation revealed that the smallest p-value across 1000 tests was 0.000475, which is highly statistically significant by conventional standards. Additionally, 51 out of 1000 tests (5.1%) produced p-values below 0.05, closely matching the expected 5% false positive rate. These results demonstrate the fundamental problem with multiple testing. Even when we know with absolute certainty that the true coefficient is zero, repeated testing will inevitably produce apparently significant results.
The false positive rate of approximately 5% aligns perfectly with theoretical expectations, confirming that each individual test maintains the correct Type I error rate. However, the critical issue emerges when considering the probability of obtaining at least one false positive across all 1000 tests. Using the formula P(at least one false positive) = 1 - (0.95)^1000, this probability approaches 100%, making it virtually guaranteed that we will find at least one "significant" result purely by chance.
To answer the question, if the p-value is less than 0.05, does this mean the coefficient actually is nonzero: no. In this simulation, even though we made sure that W has zero effect on Y, we still observed 51 instances where p < 0.05. These represent false positives. The smallest p-value of 0.000475, despite being highly significant, is meaningless because it emerged from a process designed to have no true effect. The fundamental problem with repeating analyses lies in the inflation of the family-wise error rate. While each individual test maintains a 5% false positive rate, conducting multiple tests dramatically increases the probability of finding at least one false positive. This creates several serious issues: researchers might engage in "cherry-picking" by selecting only the most significant results, the overall false discovery rate becomes much higher than the nominal 5% level, and the practice enables "p-hacking" or "data dredging" where researchers perform multiple analyses until they find significance. The smallest p-value from many tests does not represent evidence of a true relationship but rather the expected outcome of sampling variability across multiple trials.

Homework Reflection 8

Include the code you used to solve the two coding quiz problems and write about the obstacles / challenges / insights you encountered while solving them.
