Homework Reflection 5

1. Draw a diagram for the following negative feedback loop:

Sweating causes body temperature to decrease.  High body temperature causes sweating.

A negative feedback loop means that one thing increases another while the second thing decreases the first.

Remember that we are using directed acyclic graphs where two things cannot directly cause each other.

Answer: 
To represent the negative feedback loop between body temperature and sweating in a DAG, we use time subscripts to break the cycle:
Temp(t) → Sweat(t+1): High body temperature at time t causes sweating at time t+1
Sweat(t) → Temp(t+1): Sweating at time t decreases body temperature at time t+1
This maintains the feedback relationship while creating a valid DAG with no cycles. The diagram shows how each variable at one time point affects the other variable at the next time point, preserving the causal logic while avoiding the prohibited cycle.
![HWReflections5-8.png](HWReflections5-8.png)

2. Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.
An example of a positive feedback loop would be: Social Media Engagement
This works by:
More posts → More followers (quality content attracts audience) -> More followers → More motivation to post (larger audience provides encouragement)
Both variables reinforce each other upward, leading to exponential growth



3. Draw a diagram for the following situation:

Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.
Bears eat deer, decreasing their population.
Deer eat flowers, decreasing their population.

Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.

Identify a backdoor path with one or more confounders for the relationship between deer and flowers.
![alt text](HWReflections5-8.png)
Causal relationships:
Lightning → Deer/Bears (negative): Storms frighten animals away
Lightning → Flowers (positive): Storms promote flower growth
Bears → Deer (negative): Bears eat deer
Deer → Flowers (negative): Deer eat flowers

Dataset that simulates this situation:
# Lightning storms (Poisson distribution, avg 3 per year)
lightning = np.random.poisson(3, n)

# Lightning effects with noise
deer_pop = max(1, baseline - lightning*5 + noise)     # Lightning scares deer
bear_pop = max(1, baseline - lightning*2 + noise)     # Lightning scares bears  
flower_pop = max(1, baseline + lightning*10 + noise)  # Lightning helps flowers

# Predation effects with noise
deer_pop = max(1, deer_pop - bear_pop*0.3 + noise)    # Bears eat deer
flower_pop = max(1, flower_pop - deer_pop*0.8 + noise) # Deer eat flowers



Backdoor path identification:
For the relationship Deer → Flowers, there is a confounding backdoor path:
Deer ← Lightning → Flowers

Lightning is a confounder because it affects both deer (negatively) and flowers (positively), creating a spurious association. To estimate the true causal effect of deer on flowers, we must control for lightning storms.

4. Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?
![alt text](HWReflections5-8.png)
Variables:
Treatment (X): Education Level
Outcome (Y): Annual Income
Confounder: Family Wealth (affects both education and income)
Collider: Job Satisfaction (affected by both education and income)
Mediator: Professional Networking

Relationships:
Confounder - Family Wealth:
Family Wealth → Education (wealthy families afford better education)
Family Wealth → Income (inheritance, connections, opportunities)
Creates backdoor path: Education ← Family Wealth → Income

Collider - Job Satisfaction:
Education → Job Satisfaction (education brings fulfillment)
Income → Job Satisfaction (money reduces financial stress)
Controlling for job satisfaction would introduce collider bias
Mediator - Professional Networking:
Education → Networking → Income (causal pathway)



Homework Reflection 6

1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)
Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.
(Please explain / justify this answer, or give a different one if you can think of one.)

The fundamental problem with computing the Marginal Treatment Effect by simply taking the maximum difference is that this approach is highly susceptible to random noise and sampling variability, leading to systematically inflated estimates. This occurs because when we select the most extreme observation from a distribution of treatment effects. We essentially cherry-pick an outlier that likely contains both the true treatment effect and a significant amount of random error. The statistical problem manifests in several ways: selection bias (systematically choosing the most extreme case), noise amplification (extreme values typically contain more measurement error), poor generalizability (one extreme case may not represent the true population MTE), and high variance across different samples. This is analogous to the multiple testing problem in statistics, where examining many comparisons produces extreme results purely by chance, making the maximum-based MTE an unreliable estimator of the true marginal treatment effect.

2. Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.
Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.
(Either code this answer or choose a different one.)

The most effective solution to remedy the noise amplification problem is to use a percentile-based approach instead of the strict maximum, specifically employing the 90th percentile as a robust estimator of the Marginal Treatment Effect (like suggested). We can simply start by replacing the problematic maximum calculation:

mte = np.max(treatment_effects)  # Returns 2.1725 (inflated by noise)
robust_mte = np.percentile(treatment_effects, 90)  # Returns 1.9280

This approach yields an MTE of 1.9280, which is 0.2445 units lower than the problematic maximum-based estimate of 2.1725, suggesting that nearly 11% of the original estimate was likely due to noise. The 90th percentile method works effectively because it still captures individuals with high treatment effects while systematically excluding the top 10% most extreme cases that are most likely to be inflated by random variation.
Additionally, several alternative robust solutions can be implemented with simple code modifications. The average of the top K effects approach reduces the influence of any single extreme observation:

# Average of top 5 effects (returns 2.1454)
sorted_effects = sorted(treatment_effects, reverse=True)
top_k_average = np.mean(sorted_effects[:5])

A trimmed maximum approach removes extreme outliers before taking the maximum:
# Trimmed maximum (remove top/bottom 5%, returns ~2.0150)
def trimmed_max(effects, trim_percent=5):
    lower_bound = np.percentile(effects, trim_percent)
    upper_bound = np.percentile(effects, 100 - trim_percent)
    trimmed_effects = [x for x in effects if lower_bound <= x <= upper_bound]
    return np.max(trimmed_effects)
trimmed_mte = trimmed_max(treatment_effects, 5)

For a more comprehensive approach, bootstrap analysis can quantify the uncertainty in MTE estimates:
# Bootstrap confidence intervals
def bootstrap_mte(treated_data, untreated_data, n_bootstrap=500):
    bootstrap_mtes = []
    for _ in range(n_bootstrap):
        # Resample both groups
        treated_sample = treated_data.sample(n=len(treated_data), replace=True)
        untreated_sample = untreated_data.sample(n=len(untreated_data), replace=True)
        
        # Calculate MTE for bootstrap sample
        # [matching code here...]
        bootstrap_mtes.append(sample_mte)
    
    return np.mean(bootstrap_mtes), np.std(bootstrap_mtes)

bootstrap_mean, bootstrap_std = bootstrap_mte(treated, untreated)
# Returns: mean=2.1614, std=0.0146

The recommended implementation combines simplicity with statistical robustness:
def robust_marginal_treatment_effect(treated_data, untreated_data, percentile=90):
    # Fit nearest neighbors on treated group
    nn = NearestNeighbors(n_neighbors=1, metric='euclidean')
    nn.fit(treated_data[['Z']].values)
    
    # Calculate treatment effects for all untreated observations
    treatment_effects = []
    for idx, untreated_row in untreated_data.iterrows():
        distances, indices = nn.kneighbors([[untreated_row['Z']]])
        nearest_treated = treated_data.iloc[indices[0][0]]
        effect = nearest_treated['Y'] - untreated_row['Y']
        treatment_effects.append(effect)
    
    # Use percentile instead of maximum to reduce noise
    return np.percentile(treatment_effects, percentile)

# Usage:
robust_mte = robust_marginal_treatment_effect(treated, untreated, percentile=90)
# Returns 1.9280 vs 2.1725 for maximum-based approach

This percentile-based approach provides the optimal balance between capturing meaningful high treatment effects and avoiding the statistical pitfalls of extreme value selection, resulting in a more reliable and generalizable estimate of the true Marginal Treatment Effect with minimal code modification required.

Homework Reflection 7

1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between $$X$$ and $$Y$$ is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.

I created a linear regression model to demonstrate how omitting a confounder leads to biased estimates of the relationship between a treatment variable (X) and outcome variable (Y). The scenario that I'm modeling is the effect of study hours per week on test scores, with intelligence serving as an unobserved confounder.

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Set random seed for reproducibility
np.random.seed(42)
n = 1000

# True model coefficients
beta_0 = 50    # Baseline test score
beta_x = 2     # True effect of study hours on test score
beta_c = 15    # Effect of intelligence on test score
gamma_0 = 5    # Baseline study hours
gamma_c = 3    # Effect of intelligence on study hours

# Generate the confounder (intelligence)
C = np.random.normal(0, 1, n)

# Generate X (study hours) - influenced by intelligence
X = gamma_0 + gamma_c * C + np.random.normal(0, 1, n)

# Generate Y (test score) - influenced by both study hours and intelligence
Y = beta_0 + beta_x * X + beta_c * C + np.random.normal(0, 2, n)

# Fit the correct model (includes confounder)
correct_model = LinearRegression()
correct_model.fit(np.column_stack([X, C]), Y)

# Fit the incorrect model (omits confounder)
incorrect_model = LinearRegression()
incorrect_model.fit(X.reshape(-1, 1), Y)

print(f"True effect of study hours: {beta_x}")
print(f"Estimated effect (correct model): {correct_model.coef_[0]:.4f}")
print(f"Estimated effect (incorrect model): {incorrect_model.coef_[0]:.4f}")
print(f"Bias: {incorrect_model.coef_[0] - beta_x:.4f}")

The analysis reveals that the true correlation between study hours and test scores is severely overestimated when the intelligence confounder is omitted from the model. Specifically, the true effect of study hours on test scores is 2 points per additional hour studied. When intelligence is properly controlled for in the regression, the estimated effect is 1.98 points (very close to the true value). However, when intelligence is omitted from the model, the estimated effect jumps dramatically to 6.54 points per hour. This is an overestimation of 227%.
This substantial overestimation occurs because intelligence acts as a positive confounder that influences both the treatment (study hours) and the outcome (test scores) in the same direction. Students with higher intelligence tend to study more hours (γc = 3) and also tend to achieve higher test scores independently of their study time (βc = 15). When the regression model omits intelligence, it incorrectly attributes the portion of test score variation that is actually due to intelligence differences to the study hours variable instead.
The mathematical foundation for this bias follows the omitted variable bias formula: Bias = (βc × γc) / Var(X), where βc is the effect of the confounder on the outcome, γc is the effect of the confounder on the treatment, and Var(X) is the variance of the treatment variable. In this case, the theoretical bias is (15 × 3) / 9.38 = 4.80, which closely matches the observed bias of 4.54. Since both βc and γc are positive, their product creates positive bias, leading to overestimation of the true relationship. This example illustrates a fundamental principle in causal inference: when a confounder has effects in the same direction on both the treatment and outcome variables, omitting it from the model will lead to overestimation of the treatment effect. Conversely, if the confounder had opposite-direction effects (positive on one variable, negative on the other), we would observe underestimation. The magnitude of bias depends on the strength of the confounder's relationships with both variables and the variance of the treatment variable. This demonstrates why proper identification and control of confounders is crucial for obtaining unbiased estimates of causal relationships in observational data.

2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.

W = [noise]
X = [noise]
Y = 2 * X + [noise]

And compute the p-value of a coefficient - in this case, the coefficient of W.  
(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)
If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)
Run the analysis 1000 times and report the best (smallest) p-value.  
If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?

I conducted a simulation to demonstrate the multiple testing problem by performing 1000 independent linear regression analyses where one coefficient is known to be exactly zero. The model structure had variable W having no effect on the outcome Y, allowing us to identify false positive results when p-values fall below 0.05.
# imports
import numpy as np
from sklearn.linear_model import LinearRegression
from scipy import stats

def run_single_regression():
    n = 100  # Sample size
    
    # Generate data where W has NO effect on Y
    W = np.random.normal(0, 1, n)  # W is just noise
    X = np.random.normal(0, 1, n)  # X is just noise
    Y = 2 * X + np.random.normal(0, 1, n)  # Y depends only on X, not W
    
    # Fit regression: Y = β₀ + β₁W + β₂X + ε
    design_matrix = np.column_stack([W, X])
    model = LinearRegression()
    model.fit(design_matrix, Y)
    
    # Calculate p-value for W coefficient using t-test
    y_pred = model.predict(design_matrix)
    residuals = Y - y_pred
    df_resid = n - 3  # degrees of freedom
    mse = np.sum(residuals**2) / df_resid
    
    full_design = np.column_stack([np.ones(n), design_matrix])
    cov_matrix = mse * np.linalg.inv(full_design.T @ full_design)
    std_errors = np.sqrt(np.diag(cov_matrix))
    
    w_coefficient = model.coef_[0]  # Coefficient of W
    w_std_error = std_errors[1]    # Standard error of W coefficient
    t_stat = w_coefficient / w_std_error
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df_resid))  # Two-tailed test
    
    return p_value, w_coefficient

# Running 1000 independent analyses
n_simulations = 1000
p_values = []
coefficients = []

for i in range(n_simulations):
    p_val, coef = run_single_regression()
    p_values.append(p_val)
    coefficients.append(coef)

p_values = np.array(p_values)
min_p_value = np.min(p_values)
significant_count = np.sum(p_values < 0.05)
false_positive_rate = significant_count / n_simulations

print(f"Smallest p-value found: {min_p_value:.6f}")
print(f"Number of p-values < 0.05: {significant_count}")
print(f"False positive rate: {false_positive_rate:.3f}")

The simulation revealed that the smallest p-value across 1000 tests was 0.000475, which is highly statistically significant by conventional standards. Additionally, 51 out of 1000 tests (5.1%) produced p-values below 0.05, closely matching the expected 5% false positive rate. These results demonstrate the fundamental problem with multiple testing. Even when we know with absolute certainty that the true coefficient is zero, repeated testing will inevitably produce apparently significant results.
The false positive rate of approximately 5% aligns perfectly with theoretical expectations, confirming that each individual test maintains the correct Type I error rate. However, the critical issue emerges when considering the probability of obtaining at least one false positive across all 1000 tests. Using the formula P(at least one false positive) = 1 - (0.95)^1000, this probability approaches 100%, making it virtually guaranteed that we will find at least one "significant" result purely by chance.
To answer the question, if the p-value is less than 0.05, does this mean the coefficient actually is nonzero: no. In this simulation, even though we made sure that W has zero effect on Y, we still observed 51 instances where p < 0.05. These represent false positives. The smallest p-value of 0.000475, despite being highly significant, is meaningless because it emerged from a process designed to have no true effect. The fundamental problem with repeating analyses lies in the inflation of the family-wise error rate. While each individual test maintains a 5% false positive rate, conducting multiple tests dramatically increases the probability of finding at least one false positive. This creates several serious issues: researchers might engage in "cherry-picking" by selecting only the most significant results, the overall false discovery rate becomes much higher than the nominal 5% level, and the practice enables "p-hacking" or "data dredging" where researchers perform multiple analyses until they find significance. The smallest p-value from many tests does not represent evidence of a true relationship but rather the expected outcome of sampling variability across multiple trials.

Homework Reflection 8

Include the code you used to solve the two coding quiz problems and write about the obstacles / challenges / insights you encountered while solving them.

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from scipy.spatial.distance import mahalanobis
# Load the datasets
data1 = pd.read_csv('homework_8.1.csv')  # For questions 1 and 2
data2 = pd.read_csv('homework_8.2.csv')  # For questions 3 and 4
print(f"Dataset 1 has {len(data1)} rows")
print(f"Dataset 2 has {len(data2)} rows")
QUESTION 1: Average Treatment Effect with Inverse Probability Weighting

# Step 1: Look at our data
print("Data:")
print(data1.head())
print(f"\nColumns: {list(data1.columns)}")

# Count how many people got treatment vs control
treated_people = data1[data1['X'] == 1]
control_people = data1[data1['X'] == 0]
print(f"\nTreated people (X=1): {len(treated_people)}")
print(f"Control people (X=0): {len(control_people)}")

# Step 2: Estimate propensity scores using logistic regression
print("\nStep 2: Estimating the propensity scores")
# We need to predict X (treatment) from Z (the confounder)

# Prepare the data for logistic regression
Z_values = data1[['Z']]  # Features (just Z)
X_values = data1['X']    # What we want to predict (treatment assignment)

# Fit the logistic regression model
logistic_model = LogisticRegression()
logistic_model.fit(Z_values, X_values)

# Get propensity scores (probability of getting treatment)
propensity_scores = logistic_model.predict_proba(Z_values)[:, 1]

print(f"Propensity scores calculated!")
print(f"Minimum propensity score: {min(propensity_scores):.4f}")
print(f"Maximum propensity score: {max(propensity_scores):.4f}")
print(f"Average propensity score: {np.mean(propensity_scores):.4f}")

# Step 3: Calculate inverse probability weights
print("\nStep 3: Calculate the weights")
weights = []

for i in range(len(data1)):
    person = data1.iloc[i]
    prop_score = propensity_scores[i]
    
    if person['X'] == 1:  # If person got treatment
        weight = 1 / prop_score  # Weight = 1 / P(treatment)
    else:  # If person got control
        weight = 1 / (1 - prop_score)  # Weight = 1 / P(control)
    
    weights.append(weight)

weights = np.array(weights)
print(f"Weights calculated!")
print(f"Minimum weight: {min(weights):.4f}")
print(f"Maximum weight: {max(weights):.4f}")

# Step 4: Calculate weighted average treatment effect
print("\nStep 4: Calculating the ATE")

# Calculate weighted means for treated and control groups
treated_outcomes = []
treated_weights = []
control_outcomes = []
control_weights = []

for i in range(len(data1)):
    person = data1.iloc[i]
    if person['X'] == 1:  # Treated person
        treated_outcomes.append(person['Y'])
        treated_weights.append(weights[i])
    else:  # Control person
        control_outcomes.append(person['Y'])
        control_weights.append(weights[i])

# Calculate weighted averages
weighted_treated_mean = np.average(treated_outcomes, weights=treated_weights)
weighted_control_mean = np.average(control_outcomes, weights=control_weights)

# The ATE is the difference
ate = weighted_treated_mean - weighted_control_mean

print(f"Weighted average outcome for treated group: {weighted_treated_mean:.4f}")
print(f"Weighted average outcome for control group: {weighted_control_mean:.4f}")
print(f"Average Treatment Effect (ATE): {ate:.4f}")
print(f"ATE rounded to 1 decimal: {ate:.1f}")

QUESTION 2: Propensity Scores of First Three Items
first_three_scores = propensity_scores[:3]
print("The first three propensity scores are:")
for i in range(3):
    print(f"Person {i+1}: {first_three_scores[i]:.2f}")

print(f"\nAnswer: [{first_three_scores[0]:.2f}, {first_three_scores[1]:.2f}, {first_three_scores[2]:.2f}]")
QUESTION 3: Mahalanobis Distance Matching
# Step 1: Look at the second dataset
print("Dataset 2:")
print(data2.head())
print(f"Columns: {list(data2.columns)}")

# Separate treated and untreated people
treated_data = data2[data2['X'] == 1]
untreated_data = data2[data2['X'] == 0]
print(f"\nTreated people: {len(treated_data)}")
print(f"Untreated people: {len(untreated_data)}")

# Step 2: Calculate covariance matrix for Z1 and Z2
print("\nStep 2: Covariance matrix")

# Get all Z1 and Z2 values
all_z1 = data2['Z1'].values
all_z2 = data2['Z2'].values

# Create a matrix with Z1 and Z2 columns
Z_matrix = np.column_stack([all_z1, all_z2])

# Calculate covariance matrix
cov_matrix = np.cov(Z_matrix.T)  # .T means transpose
print("Covariance matrix:")
print(cov_matrix)

# Calculate inverse covariance matrix
inv_cov_matrix = np.linalg.inv(cov_matrix)
print("\nInverse covariance matrix:")
print(inv_cov_matrix)

# Step 3: Match each treated person to nearest untreated person
print("\nStep 3: Finding matches using Mahalanobis distance")

treatment_effects = []

# Go through each treated person
for treated_index, treated_person in treated_data.iterrows():
    treated_z = [treated_person['Z1'], treated_person['Z2']]
    
    best_match_distance = float('inf')  # Start with infinite distance
    best_match_outcome = None
    
    # Compare with every untreated person
    for untreated_index, untreated_person in untreated_data.iterrows():
        untreated_z = [untreated_person['Z1'], untreated_person['Z2']]
        
        # Calculate Mahalanobis distance
        distance = mahalanobis(treated_z, untreated_z, inv_cov_matrix)
        
        # If this is the closest match so far, remember it
        if distance < best_match_distance:
            best_match_distance = distance
            best_match_outcome = untreated_person['Y']
    
    # Calculate treatment effect for this matched pair
    individual_effect = treated_person['Y'] - best_match_outcome
    treatment_effects.append(individual_effect)

# Step 4: Calculate average treatment effect
ate_mahalanobis = np.mean(treatment_effects)

print(f"Matched {len(treatment_effects)} treated people to untreated people")
print(f"Average Treatment Effect: {ate_mahalanobis:.4f}")
print(f"ATE rounded to 1 decimal: {ate_mahalanobis:.1f}")
QUESTION 4: Finding the Treated Item with Least Common Support
worst_support_distance = 0
worst_support_person = None
worst_support_z1 = None
worst_support_z2 = None

# Go through each treated person
for treated_index, treated_person in treated_data.iterrows():
    treated_z = [treated_person['Z1'], treated_person['Z2']]
    
    # Find the distance to the CLOSEST untreated person
    closest_distance = float('inf')
    
    for untreated_index, untreated_person in untreated_data.iterrows():
        untreated_z = [untreated_person['Z1'], untreated_person['Z2']]
        distance = mahalanobis(treated_z, untreated_z, inv_cov_matrix)
        
        if distance < closest_distance:
            closest_distance = distance
    
    # If this person's closest match is farther than anyone else's,
    # they have the least common support
    if closest_distance > worst_support_distance:
        worst_support_distance = closest_distance
        worst_support_person = treated_person
        worst_support_z1 = treated_person['Z1']
        worst_support_z2 = treated_person['Z2']

print(f"Treated person with least common support")
print(f"Their Z1 value: {worst_support_z1:.1f}")
print(f"Their Z2 value: {worst_support_z2:.1f}")
print(f"Distance to nearest untreated person: {worst_support_distance:.4f}")

print(f"Question 1 - ATE closest to: {ate:.1f}")
print(f"Question 2 - First three propensity scores: [{first_three_scores[0]:.2f}, {first_three_scores[1]:.2f}, {first_three_scores[2]:.2f}]")
print(f"Question 3 - ATE closest to: {ate_mahalanobis:.1f}")
print(f"Question 4 - Z1, Z2 values: ({worst_support_z1:.1f}, {worst_support_z2:.1f})")

While working through these problems, I encountered several key challenges that deepened my understanding of causal inference methods. The most difficult concept was understanding propensity scores and inverse probability weighting. It took time to grasp that we're predicting treatment assignment from confounders (not outcomes from treatment) and that the weights create a "pseudo-population" where treatment is independent of confounders. Implementing the Mahalanobis distance matching was technically challenging because I had to carefully manage the nested loops and understand why we use the full covariance matrix rather than simple Euclidean distance. The concept of "common support" in Question 4 was initially confusing until I realized it identifies treated units that are most dissimilar from any control units, making causal inference less reliable for these cases. Throughout the quiz, I made sure to add diagnostic print statements and checking intermediate results, especially since different methods (IPW vs. Mahalanobis) gave quite different estimates (2.3 vs. 3.4), highlighting how sensitive causal inference can be to methodological choices. This reinforced that these methods are approximations to randomized experiments, each with important assumptions and limitations.