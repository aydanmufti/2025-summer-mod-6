{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d127569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial.distance import mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0865b250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 has 1000 rows\n",
      "Dataset 2 has 1000 rows\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "data1 = pd.read_csv('homework_8.1.csv')  # For questions 1 and 2\n",
    "data2 = pd.read_csv('homework_8.2.csv')  # For questions 3 and 4\n",
    "print(f\"Dataset 1 has {len(data1)} rows\")\n",
    "print(f\"Dataset 2 has {len(data2)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616dc33",
   "metadata": {},
   "source": [
    "QUESTION 1: Using homework_8.1.csv, find the Average treatment effect with inverse probability weighting. Then, include your code and a written explanation of your work (mentioning any choices or strategies you made in writing the code) in your homework reflection.  \n",
    "\n",
    "\n",
    "\n",
    "Here are some steps to follow: \n",
    "\n",
    "\n",
    "\n",
    "* Estimate the propensity scores using logistic regression. Fit the model so that the Z values predict ﻿X﻿. \n",
    "\n",
    "* Use the model to predict the propensity scores (e.g., using predict_proba if you are using sklearn). \n",
    "\n",
    "* Calculate inverse probability weights (﻿1 over P﻿ for ﻿X equals 1﻿ and ﻿fraction numerator 1 over denominator 1 minus P end fraction﻿ for ﻿X equals 0﻿). \n",
    "\n",
    "* Estimate the average treatment effect (the Y difference between ﻿X equals 1﻿ and ﻿X equals 0﻿, using the appropriate weights for each). \n",
    "\n",
    "\n",
    "\n",
    "Then, the ATE is closest to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa864170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "   Unnamed: 0  X         Y         Z\n",
      "0           0  1  4.109218  1.764052\n",
      "1           1  0  2.259504  0.400157\n",
      "2           2  0 -0.647584  0.978738\n",
      "3           3  0  2.106071  2.240893\n",
      "4           4  1  3.583464  1.867558\n",
      "\n",
      "Columns: ['Unnamed: 0', 'X', 'Y', 'Z']\n",
      "\n",
      "Treated people (X=1): 481\n",
      "Control people (X=0): 519\n",
      "\n",
      "Step 2: Estimating the propensity scores\n",
      "Propensity scores calculated!\n",
      "Minimum propensity score: 0.0480\n",
      "Maximum propensity score: 0.9322\n",
      "Average propensity score: 0.4810\n",
      "\n",
      "Step 3: Calculate the weights\n",
      "Weights calculated!\n",
      "Minimum weight: 1.0505\n",
      "Maximum weight: 12.7167\n",
      "\n",
      "Step 4: Calculating the ATE\n",
      "Weighted average outcome for treated group: 2.2367\n",
      "Weighted average outcome for control group: -0.0376\n",
      "Average Treatment Effect (ATE): 2.2743\n",
      "ATE rounded to 1 decimal: 2.3\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Look at our data\n",
    "print(\"Data:\")\n",
    "print(data1.head())\n",
    "print(f\"\\nColumns: {list(data1.columns)}\")\n",
    "\n",
    "# Count how many people got treatment vs control\n",
    "treated_people = data1[data1['X'] == 1]\n",
    "control_people = data1[data1['X'] == 0]\n",
    "print(f\"\\nTreated people (X=1): {len(treated_people)}\")\n",
    "print(f\"Control people (X=0): {len(control_people)}\")\n",
    "\n",
    "# Step 2: Estimate propensity scores using logistic regression\n",
    "print(\"\\nStep 2: Estimating the propensity scores\")\n",
    "# We need to predict X (treatment) from Z (the confounder)\n",
    "\n",
    "# Prepare the data for logistic regression\n",
    "Z_values = data1[['Z']]  # Features (just Z)\n",
    "X_values = data1['X']    # What we want to predict (treatment assignment)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(Z_values, X_values)\n",
    "\n",
    "# Get propensity scores (probability of getting treatment)\n",
    "propensity_scores = logistic_model.predict_proba(Z_values)[:, 1]\n",
    "\n",
    "print(f\"Propensity scores calculated!\")\n",
    "print(f\"Minimum propensity score: {min(propensity_scores):.4f}\")\n",
    "print(f\"Maximum propensity score: {max(propensity_scores):.4f}\")\n",
    "print(f\"Average propensity score: {np.mean(propensity_scores):.4f}\")\n",
    "\n",
    "# Step 3: Calculate inverse probability weights\n",
    "print(\"\\nStep 3: Calculate the weights\")\n",
    "weights = []\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    person = data1.iloc[i]\n",
    "    prop_score = propensity_scores[i]\n",
    "    \n",
    "    if person['X'] == 1:  # If person got treatment\n",
    "        weight = 1 / prop_score  # Weight = 1 / P(treatment)\n",
    "    else:  # If person got control\n",
    "        weight = 1 / (1 - prop_score)  # Weight = 1 / P(control)\n",
    "    \n",
    "    weights.append(weight)\n",
    "\n",
    "weights = np.array(weights)\n",
    "print(f\"Weights calculated!\")\n",
    "print(f\"Minimum weight: {min(weights):.4f}\")\n",
    "print(f\"Maximum weight: {max(weights):.4f}\")\n",
    "\n",
    "# Step 4: Calculate weighted average treatment effect\n",
    "print(\"\\nStep 4: Calculating the ATE\")\n",
    "\n",
    "# Calculate weighted means for treated and control groups\n",
    "treated_outcomes = []\n",
    "treated_weights = []\n",
    "control_outcomes = []\n",
    "control_weights = []\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    person = data1.iloc[i]\n",
    "    if person['X'] == 1:  # Treated person\n",
    "        treated_outcomes.append(person['Y'])\n",
    "        treated_weights.append(weights[i])\n",
    "    else:  # Control person\n",
    "        control_outcomes.append(person['Y'])\n",
    "        control_weights.append(weights[i])\n",
    "\n",
    "# Calculate weighted averages\n",
    "weighted_treated_mean = np.average(treated_outcomes, weights=treated_weights)\n",
    "weighted_control_mean = np.average(control_outcomes, weights=control_weights)\n",
    "\n",
    "# The ATE is the difference\n",
    "ate = weighted_treated_mean - weighted_control_mean\n",
    "\n",
    "print(f\"Weighted average outcome for treated group: {weighted_treated_mean:.4f}\")\n",
    "print(f\"Weighted average outcome for control group: {weighted_control_mean:.4f}\")\n",
    "print(f\"Average Treatment Effect (ATE): {ate:.4f}\")\n",
    "print(f\"ATE rounded to 1 decimal: {ate:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5872d",
   "metadata": {},
   "source": [
    "QUESTION 2: The propensity scores of the first three items in the dataframe are closest to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7077808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three propensity scores are:\n",
      "Person 1: 0.84\n",
      "Person 2: 0.58\n",
      "Person 3: 0.71\n",
      "\n",
      "Answer: [0.84, 0.58, 0.71]\n"
     ]
    }
   ],
   "source": [
    "first_three_scores = propensity_scores[:3]\n",
    "print(\"The first three propensity scores are:\")\n",
    "for i in range(3):\n",
    "    print(f\"Person {i+1}: {first_three_scores[i]:.2f}\")\n",
    "\n",
    "print(f\"\\nAnswer: [{first_three_scores[0]:.2f}, {first_three_scores[1]:.2f}, {first_three_scores[2]:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13297165",
   "metadata": {},
   "source": [
    "QUESTION 3: Using homework_8.2.csv, match all treated items to the single nearest untreated item using the Mahalanobis distance. (Do this with replacement — the same untreated item can be used again.) \n",
    "\n",
    "\n",
    "\n",
    "* Use the Mahalanobis function from scipy.spatial.distance \n",
    "\n",
    "* For the inverse covariance matrix, use all ﻿Z 1﻿ values and all ﻿Z 2﻿ values, make them into a ﻿2 x N﻿ matrix, find its ﻿2 x 2﻿ covariance, and invert. \n",
    "\n",
    "\n",
    "\n",
    "Then, the ATE is closest to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf41be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2:\n",
      "   Unnamed: 0  X         Y        Z1        Z2\n",
      "0           0  1  4.652085  1.764052  2.320015\n",
      "1           1  1  2.590221  0.400157  1.292631\n",
      "2           2  1  3.898981  0.978738  0.556423\n",
      "3           3  1  5.857179  2.240893  2.345607\n",
      "4           4  1  3.647489  1.867558  2.095611\n",
      "Columns: ['Unnamed: 0', 'X', 'Y', 'Z1', 'Z2']\n",
      "\n",
      "Treated people: 483\n",
      "Untreated people: 517\n",
      "\n",
      "Step 2: Covariance matrix\n",
      "Covariance matrix:\n",
      "[[0.97520967 0.94507003]\n",
      " [0.94507003 1.85320242]]\n",
      "\n",
      "Inverse covariance matrix:\n",
      "[[ 2.02734407 -1.03387633]\n",
      " [-1.03387633  1.06684813]]\n",
      "\n",
      "Step 3: Finding matches using Mahalanobis distance\n",
      "Matched 483 treated people to untreated people\n",
      "Average Treatment Effect: 3.4377\n",
      "ATE rounded to 1 decimal: 3.4\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Look at the second dataset\n",
    "print(\"Dataset 2:\")\n",
    "print(data2.head())\n",
    "print(f\"Columns: {list(data2.columns)}\")\n",
    "\n",
    "# Separate treated and untreated people\n",
    "treated_data = data2[data2['X'] == 1]\n",
    "untreated_data = data2[data2['X'] == 0]\n",
    "print(f\"\\nTreated people: {len(treated_data)}\")\n",
    "print(f\"Untreated people: {len(untreated_data)}\")\n",
    "\n",
    "# Step 2: Calculate covariance matrix for Z1 and Z2\n",
    "print(\"\\nStep 2: Covariance matrix\")\n",
    "\n",
    "# Get all Z1 and Z2 values\n",
    "all_z1 = data2['Z1'].values\n",
    "all_z2 = data2['Z2'].values\n",
    "\n",
    "# Create a matrix with Z1 and Z2 columns\n",
    "Z_matrix = np.column_stack([all_z1, all_z2])\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = np.cov(Z_matrix.T)  # .T means transpose\n",
    "print(\"Covariance matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Calculate inverse covariance matrix\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "print(\"\\nInverse covariance matrix:\")\n",
    "print(inv_cov_matrix)\n",
    "\n",
    "# Step 3: Match each treated person to nearest untreated person\n",
    "print(\"\\nStep 3: Finding matches using Mahalanobis distance\")\n",
    "\n",
    "treatment_effects = []\n",
    "\n",
    "# Go through each treated person\n",
    "for treated_index, treated_person in treated_data.iterrows():\n",
    "    treated_z = [treated_person['Z1'], treated_person['Z2']]\n",
    "    \n",
    "    best_match_distance = float('inf')  # Start with infinite distance\n",
    "    best_match_outcome = None\n",
    "    \n",
    "    # Compare with every untreated person\n",
    "    for untreated_index, untreated_person in untreated_data.iterrows():\n",
    "        untreated_z = [untreated_person['Z1'], untreated_person['Z2']]\n",
    "        \n",
    "        # Calculate Mahalanobis distance\n",
    "        distance = mahalanobis(treated_z, untreated_z, inv_cov_matrix)\n",
    "        \n",
    "        # If this is the closest match so far, remember it\n",
    "        if distance < best_match_distance:\n",
    "            best_match_distance = distance\n",
    "            best_match_outcome = untreated_person['Y']\n",
    "    \n",
    "    # Calculate treatment effect for this matched pair\n",
    "    individual_effect = treated_person['Y'] - best_match_outcome\n",
    "    treatment_effects.append(individual_effect)\n",
    "\n",
    "# Step 4: Calculate average treatment effect\n",
    "ate_mahalanobis = np.mean(treatment_effects)\n",
    "\n",
    "print(f\"Matched {len(treatment_effects)} treated people to untreated people\")\n",
    "print(f\"Average Treatment Effect: {ate_mahalanobis:.4f}\")\n",
    "print(f\"ATE rounded to 1 decimal: {ate_mahalanobis:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdc955",
   "metadata": {},
   "source": [
    "QUESTION 4:  Find the nearest Z1 and Z2 values of the treated item with the least common support (the farthest Mahalanobis distance from the untreated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12609021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treated person with least common support\n",
      "Their Z1 value: 2.7\n",
      "Their Z2 value: 0.5\n",
      "Distance to nearest untreated person: 1.3830\n"
     ]
    }
   ],
   "source": [
    "worst_support_distance = 0\n",
    "worst_support_person = None\n",
    "worst_support_z1 = None\n",
    "worst_support_z2 = None\n",
    "\n",
    "# Go through each treated person\n",
    "for treated_index, treated_person in treated_data.iterrows():\n",
    "    treated_z = [treated_person['Z1'], treated_person['Z2']]\n",
    "    \n",
    "    # Find the distance to the CLOSEST untreated person\n",
    "    closest_distance = float('inf')\n",
    "    \n",
    "    for untreated_index, untreated_person in untreated_data.iterrows():\n",
    "        untreated_z = [untreated_person['Z1'], untreated_person['Z2']]\n",
    "        distance = mahalanobis(treated_z, untreated_z, inv_cov_matrix)\n",
    "        \n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "    \n",
    "    # If this person's closest match is farther than anyone else's,\n",
    "    # they have the least common support\n",
    "    if closest_distance > worst_support_distance:\n",
    "        worst_support_distance = closest_distance\n",
    "        worst_support_person = treated_person\n",
    "        worst_support_z1 = treated_person['Z1']\n",
    "        worst_support_z2 = treated_person['Z2']\n",
    "\n",
    "print(f\"Treated person with least common support\")\n",
    "print(f\"Their Z1 value: {worst_support_z1:.1f}\")\n",
    "print(f\"Their Z2 value: {worst_support_z2:.1f}\")\n",
    "print(f\"Distance to nearest untreated person: {worst_support_distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d416c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 - ATE closest to: 2.3\n",
      "Question 2 - First three propensity scores: [0.84, 0.58, 0.71]\n",
      "Question 3 - ATE closest to: 3.4\n",
      "Question 4 - Z1, Z2 values: (2.7, 0.5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question 1 - ATE closest to: {ate:.1f}\")\n",
    "print(f\"Question 2 - First three propensity scores: [{first_three_scores[0]:.2f}, {first_three_scores[1]:.2f}, {first_three_scores[2]:.2f}]\")\n",
    "print(f\"Question 3 - ATE closest to: {ate_mahalanobis:.1f}\")\n",
    "print(f\"Question 4 - Z1, Z2 values: ({worst_support_z1:.1f}, {worst_support_z2:.1f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
