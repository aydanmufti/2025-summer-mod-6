{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc65608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of detecting nonzero effect: 86.6%\n",
      "\n",
      "Sample regression results:\n",
      "Intercept coefficient: 0.068\n",
      "X coefficient: 1.137\n",
      "W coefficient: -1.833\n",
      "Standard error of X: 0.308\n",
      "t-statistic for X: 3.692\n",
      "Is significant (|t| > 1.96): True\n",
      "\n",
      "Data characteristics:\n",
      "Sample size (D): 1000\n",
      "True effect of X (A): 1\n",
      "Noise in X (B): 1\n",
      "Noise in Y (C): 10\n",
      "Correlation between X and W: 0.712\n",
      "Standard deviation of X: 1.457\n",
      "Standard deviation of Y: 10.040\n",
      "\n",
      "Running multiple power calculations:\n",
      "Trial 1: 87.4%\n",
      "Trial 2: 91.8%\n",
      "Trial 3: 88.2%\n",
      "Trial 4: 88.0%\n",
      "Trial 5: 88.4%\n",
      "Trial 6: 86.4%\n",
      "Trial 7: 88.2%\n",
      "Trial 8: 88.8%\n",
      "Trial 9: 90.0%\n",
      "Trial 10: 90.6%\n",
      "\n",
      "Mean power across trials: 88.8%\n",
      "Standard deviation: 0.015\n",
      "\n",
      "Closest option:\n",
      "Option B (88%) - difference: 0.008\n",
      "\n",
      "Theoretical considerations:\n",
      "- Large sample size (n=1000) increases power\n",
      "- True effect size A=1 is moderate\n",
      "- High noise in Y (C=10) decreases power\n",
      "- Including confounder W is essential for unbiased estimation\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate(A=1, B=1, C=10, D=1000):\n",
    "    \"\"\"\n",
    "    Simulate data according to the given model:\n",
    "    W ~ N(0,1)\n",
    "    X = W + N(0,B)\n",
    "    Y = A*X - W + N(0,C)\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, D)\n",
    "    X = W + np.random.normal(0, B, D)\n",
    "    Y = A*X - W + np.random.normal(0, C, D)\n",
    "    return Y, X, W\n",
    "\n",
    "def manual_regression_with_W(Y, X, W):\n",
    "    \"\"\"\n",
    "    Manually calculate regression Y ~ X + W and return the t-statistic for X\n",
    "    \"\"\"\n",
    "    # Create design matrix with intercept, X, and W\n",
    "    n = len(Y)\n",
    "    X_matrix = np.column_stack([np.ones(n), X, W])\n",
    "    \n",
    "    # Calculate coefficients using normal equation: β = (X'X)^(-1)X'Y\n",
    "    XTX = X_matrix.T @ X_matrix\n",
    "    XTY = X_matrix.T @ Y\n",
    "    coefficients = np.linalg.solve(XTX, XTY)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    Y_pred = X_matrix @ coefficients\n",
    "    residuals = Y - Y_pred\n",
    "    \n",
    "    # Calculate residual sum of squares\n",
    "    RSS = np.sum(residuals**2)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = n - X_matrix.shape[1]  # n - number of parameters\n",
    "    \n",
    "    # Mean squared error\n",
    "    MSE = RSS / df\n",
    "    \n",
    "    # Variance-covariance matrix of coefficients\n",
    "    var_covar_matrix = MSE * np.linalg.inv(XTX)\n",
    "    \n",
    "    # Standard error of X coefficient (index 1)\n",
    "    se_X = np.sqrt(var_covar_matrix[1, 1])\n",
    "    \n",
    "    # t-statistic for X coefficient\n",
    "    t_stat = coefficients[1] / se_X\n",
    "    \n",
    "    return t_stat, coefficients, se_X\n",
    "\n",
    "def calculate_power(A=1, B=1, C=10, D=1000, n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Calculate the probability of detecting a nonzero effect of X on Y\n",
    "    (t-value > 1.96 in absolute value) through Monte Carlo simulation\n",
    "    \"\"\"\n",
    "    significant_count = 0\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Generate data\n",
    "        Y, X, W = simulate(A, B, C, D)\n",
    "        \n",
    "        # Run regression\n",
    "        t_stat, _, _ = manual_regression_with_W(Y, X, W)\n",
    "        \n",
    "        # Check if significant (|t| > 1.96)\n",
    "        if abs(t_stat) > 1.96:\n",
    "            significant_count += 1\n",
    "    \n",
    "    power = significant_count / n_simulations\n",
    "    return power\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define parameters\n",
    "A, B, C, D = 1, 1, 10, 1000\n",
    "\n",
    "# Calculate power for the given parameters\n",
    "power = calculate_power(A=A, B=B, C=C, D=D, n_simulations=1000)\n",
    "print(f\"Probability of detecting nonzero effect: {power:.1%}\")\n",
    "\n",
    "# Let's also run a single simulation to understand the model better\n",
    "Y, X, W = simulate(A=A, B=B, C=C, D=D)\n",
    "t_stat, coefficients, se_X = manual_regression_with_W(Y, X, W)\n",
    "\n",
    "print(f\"\\nSample regression results:\")\n",
    "print(f\"Intercept coefficient: {coefficients[0]:.3f}\")\n",
    "print(f\"X coefficient: {coefficients[1]:.3f}\")\n",
    "print(f\"W coefficient: {coefficients[2]:.3f}\")\n",
    "print(f\"Standard error of X: {se_X:.3f}\")\n",
    "print(f\"t-statistic for X: {t_stat:.3f}\")\n",
    "print(f\"Is significant (|t| > 1.96): {abs(t_stat) > 1.96}\")\n",
    "\n",
    "# Data characteristics\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"Sample size (D): {len(Y)}\")\n",
    "print(f\"True effect of X (A): {A}\")\n",
    "print(f\"Noise in X (B): {B}\")\n",
    "print(f\"Noise in Y (C): {C}\")\n",
    "print(f\"Correlation between X and W: {np.corrcoef(X, W)[0,1]:.3f}\")\n",
    "print(f\"Standard deviation of X: {np.std(X):.3f}\")\n",
    "print(f\"Standard deviation of Y: {np.std(Y):.3f}\")\n",
    "\n",
    "# Run multiple trials to get a more stable estimate\n",
    "print(f\"\\nRunning multiple power calculations:\")\n",
    "powers = []\n",
    "for trial in range(10):\n",
    "    np.random.seed(42 + trial)\n",
    "    power_trial = calculate_power(A=A, B=B, C=C, D=D, n_simulations=500)\n",
    "    powers.append(power_trial)\n",
    "    print(f\"Trial {trial+1}: {power_trial:.1%}\")\n",
    "\n",
    "mean_power = np.mean(powers)\n",
    "std_power = np.std(powers)\n",
    "print(f\"\\nMean power across trials: {mean_power:.1%}\")\n",
    "print(f\"Standard deviation: {std_power:.3f}\")\n",
    "\n",
    "# Compare with options\n",
    "print(f\"\\nClosest option:\")\n",
    "options = [0.98, 0.88, 0.83, 0.93]\n",
    "option_labels = ['A (98%)', 'B (88%)', 'C (83%)', 'D (93%)']\n",
    "differences = [abs(mean_power - opt) for opt in options]\n",
    "closest_idx = np.argmin(differences)\n",
    "print(f\"Option {option_labels[closest_idx]} - difference: {differences[closest_idx]:.3f}\")\n",
    "\n",
    "# Let's also do a quick theoretical check\n",
    "print(f\"\\nTheoretical considerations:\")\n",
    "print(f\"- Large sample size (n=1000) increases power\")\n",
    "print(f\"- True effect size A=1 is moderate\")\n",
    "print(f\"- High noise in Y (C=10) decreases power\")\n",
    "print(f\"- Including confounder W is essential for unbiased estimation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ff0a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of detecting nonzero effect: 86.6%\n",
      "\n",
      "Sample regression results:\n",
      "Intercept coefficient: 0.068\n",
      "X coefficient: 1.137\n",
      "W coefficient: -1.833\n",
      "Standard error of X: 0.308\n",
      "t-statistic for X: 3.692\n",
      "Is significant (|t| > 1.96): True\n",
      "\n",
      "Data characteristics:\n",
      "Sample size (D): 1000\n",
      "True effect of X (A): 1\n",
      "Noise in X (B): 1\n",
      "Noise in Y (C): 10\n",
      "Correlation between X and W: 0.712\n",
      "Standard deviation of X: 1.457\n",
      "Standard deviation of Y: 10.040\n",
      "\n",
      "Running multiple power calculations:\n",
      "Trial 1: 87.4%\n",
      "Trial 2: 91.8%\n",
      "Trial 3: 88.2%\n",
      "Trial 4: 88.0%\n",
      "Trial 5: 88.4%\n",
      "Trial 6: 86.4%\n",
      "Trial 7: 88.2%\n",
      "Trial 8: 88.8%\n",
      "Trial 9: 90.0%\n",
      "Trial 10: 90.6%\n",
      "\n",
      "Mean power across trials: 88.8%\n",
      "Standard deviation: 0.015\n",
      "\n",
      "Closest option:\n",
      "Option B (88%) - difference: 0.008\n",
      "\n",
      "============================================================\n",
      "QUESTION 2: SKEWNESS OF X COEFFICIENT ESTIMATES\n",
      "============================================================\n",
      "Skewness of X coefficient estimates: -0.040\n",
      "Mean of X coefficients: 0.995\n",
      "Standard deviation of X coefficients: 0.321\n",
      "Min coefficient: -0.316\n",
      "Max coefficient: 2.107\n",
      "\n",
      "Closest option for skewness:\n",
      "Option B (0) - difference: 0.040\n",
      "\n",
      "Multiple skewness calculations:\n",
      "Trial 1: -0.062\n",
      "Trial 2: 0.023\n",
      "Trial 3: -0.020\n",
      "Trial 4: -0.073\n",
      "Trial 5: 0.029\n",
      "\n",
      "Mean skewness across trials: -0.020\n",
      "Closest option based on mean: B (0) - difference: 0.020\n",
      "\n",
      "Interpretation:\n",
      "- Distribution is approximately symmetric (skewness ≈ 0)\n",
      "\n",
      "Theoretical expectation:\n",
      "- OLS estimates are unbiased, so E[β̂] = β = 1\n",
      "- With large sample size and normal errors, estimates should be approximately normal\n",
      "- Normal distribution has skewness = 0\n",
      "- Small deviations from 0 are expected due to sampling variability\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate(A=1, B=1, C=10, D=1000):\n",
    "    \"\"\"\n",
    "    Simulate data according to the given model:\n",
    "    W ~ N(0,1)\n",
    "    X = W + N(0,B)\n",
    "    Y = A*X - W + N(0,C)\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, D)\n",
    "    X = W + np.random.normal(0, B, D)\n",
    "    Y = A*X - W + np.random.normal(0, C, D)\n",
    "    return Y, X, W\n",
    "\n",
    "def manual_regression_with_W(Y, X, W):\n",
    "    \"\"\"\n",
    "    Manually calculate regression Y ~ X + W and return the t-statistic for X\n",
    "    \"\"\"\n",
    "    # Create design matrix with intercept, X, and W\n",
    "    n = len(Y)\n",
    "    X_matrix = np.column_stack([np.ones(n), X, W])\n",
    "    \n",
    "    # Calculate coefficients using normal equation: β = (X'X)^(-1)X'Y\n",
    "    XTX = X_matrix.T @ X_matrix\n",
    "    XTY = X_matrix.T @ Y\n",
    "    coefficients = np.linalg.solve(XTX, XTY)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    Y_pred = X_matrix @ coefficients\n",
    "    residuals = Y - Y_pred\n",
    "    \n",
    "    # Calculate residual sum of squares\n",
    "    RSS = np.sum(residuals**2)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = n - X_matrix.shape[1]  # n - number of parameters\n",
    "    \n",
    "    # Mean squared error\n",
    "    MSE = RSS / df\n",
    "    \n",
    "    # Variance-covariance matrix of coefficients\n",
    "    var_covar_matrix = MSE * np.linalg.inv(XTX)\n",
    "    \n",
    "    # Standard error of X coefficient (index 1)\n",
    "    se_X = np.sqrt(var_covar_matrix[1, 1])\n",
    "    \n",
    "    # t-statistic for X coefficient\n",
    "    t_stat = coefficients[1] / se_X\n",
    "    \n",
    "    return t_stat, coefficients, se_X\n",
    "\n",
    "def calculate_power(A=1, B=1, C=10, D=1000, n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Calculate the probability of detecting a nonzero effect of X on Y\n",
    "    (t-value > 1.96 in absolute value) through Monte Carlo simulation\n",
    "    \"\"\"\n",
    "    significant_count = 0\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Generate data\n",
    "        Y, X, W = simulate(A, B, C, D)\n",
    "        \n",
    "        # Run regression\n",
    "        t_stat, _, _ = manual_regression_with_W(Y, X, W)\n",
    "        \n",
    "        # Check if significant (|t| > 1.96)\n",
    "        if abs(t_stat) > 1.96:\n",
    "            significant_count += 1\n",
    "    \n",
    "    power = significant_count / n_simulations\n",
    "    return power\n",
    "\n",
    "def calculate_skewness(A=1, B=1, C=10, D=1000, n_simulations=5000):\n",
    "    \"\"\"\n",
    "    Calculate the skewness of the X coefficient estimates\n",
    "    \"\"\"\n",
    "    from scipy.stats import skew\n",
    "    \n",
    "    coefficients = []\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Generate data\n",
    "        Y, X, W = simulate(A, B, C, D)\n",
    "        \n",
    "        # Run regression\n",
    "        t_stat, coef, _ = manual_regression_with_W(Y, X, W)\n",
    "        \n",
    "        # Store the X coefficient (index 1)\n",
    "        coefficients.append(coef[1])\n",
    "    \n",
    "    coefficients = np.array(coefficients)\n",
    "    \n",
    "    # Calculate skewness\n",
    "    skewness = skew(coefficients)\n",
    "    \n",
    "    return skewness, coefficients\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define parameters\n",
    "A, B, C, D = 1, 1, 10, 1000\n",
    "\n",
    "# Calculate power for the given parameters\n",
    "power = calculate_power(A=A, B=B, C=C, D=D, n_simulations=1000)\n",
    "print(f\"Probability of detecting nonzero effect: {power:.1%}\")\n",
    "\n",
    "# Let's also run a single simulation to understand the model better\n",
    "Y, X, W = simulate(A=A, B=B, C=C, D=D)\n",
    "t_stat, coefficients, se_X = manual_regression_with_W(Y, X, W)\n",
    "\n",
    "print(f\"\\nSample regression results:\")\n",
    "print(f\"Intercept coefficient: {coefficients[0]:.3f}\")\n",
    "print(f\"X coefficient: {coefficients[1]:.3f}\")\n",
    "print(f\"W coefficient: {coefficients[2]:.3f}\")\n",
    "print(f\"Standard error of X: {se_X:.3f}\")\n",
    "print(f\"t-statistic for X: {t_stat:.3f}\")\n",
    "print(f\"Is significant (|t| > 1.96): {abs(t_stat) > 1.96}\")\n",
    "\n",
    "# Data characteristics\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"Sample size (D): {len(Y)}\")\n",
    "print(f\"True effect of X (A): {A}\")\n",
    "print(f\"Noise in X (B): {B}\")\n",
    "print(f\"Noise in Y (C): {C}\")\n",
    "print(f\"Correlation between X and W: {np.corrcoef(X, W)[0,1]:.3f}\")\n",
    "print(f\"Standard deviation of X: {np.std(X):.3f}\")\n",
    "print(f\"Standard deviation of Y: {np.std(Y):.3f}\")\n",
    "\n",
    "# Run multiple trials to get a more stable estimate\n",
    "print(f\"\\nRunning multiple power calculations:\")\n",
    "powers = []\n",
    "for trial in range(10):\n",
    "    np.random.seed(42 + trial)\n",
    "    power_trial = calculate_power(A=A, B=B, C=C, D=D, n_simulations=500)\n",
    "    powers.append(power_trial)\n",
    "    print(f\"Trial {trial+1}: {power_trial:.1%}\")\n",
    "\n",
    "mean_power = np.mean(powers)\n",
    "std_power = np.std(powers)\n",
    "print(f\"\\nMean power across trials: {mean_power:.1%}\")\n",
    "print(f\"Standard deviation: {std_power:.3f}\")\n",
    "\n",
    "# Compare with options\n",
    "print(f\"\\nClosest option:\")\n",
    "options = [0.98, 0.88, 0.83, 0.93]\n",
    "option_labels = ['A (98%)', 'B (88%)', 'C (83%)', 'D (93%)']\n",
    "differences = [abs(mean_power - opt) for opt in options]\n",
    "closest_idx = np.argmin(differences)\n",
    "print(f\"Option {option_labels[closest_idx]} - difference: {differences[closest_idx]:.3f}\")\n",
    "\n",
    "# Calculate skewness of X coefficient estimates\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"QUESTION 2: SKEWNESS OF X COEFFICIENT ESTIMATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "skewness_value, coefficients = calculate_skewness(A=A, B=B, C=C, D=D, n_simulations=5000)\n",
    "\n",
    "print(f\"Skewness of X coefficient estimates: {skewness_value:.3f}\")\n",
    "print(f\"Mean of X coefficients: {np.mean(coefficients):.3f}\")\n",
    "print(f\"Standard deviation of X coefficients: {np.std(coefficients):.3f}\")\n",
    "print(f\"Min coefficient: {np.min(coefficients):.3f}\")\n",
    "print(f\"Max coefficient: {np.max(coefficients):.3f}\")\n",
    "\n",
    "# Compare with options\n",
    "print(f\"\\nClosest option for skewness:\")\n",
    "skew_options = [-0.15, 0, 0.34, 0.15]\n",
    "skew_option_labels = ['A (-0.15)', 'B (0)', 'C (0.34)', 'D (0.15)']\n",
    "skew_differences = [abs(skewness_value - opt) for opt in skew_options]\n",
    "closest_skew_idx = np.argmin(skew_differences)\n",
    "print(f\"Option {skew_option_labels[closest_skew_idx]} - difference: {skew_differences[closest_skew_idx]:.3f}\")\n",
    "\n",
    "# Let's also check with multiple trials for stability\n",
    "print(f\"\\nMultiple skewness calculations:\")\n",
    "skewness_values = []\n",
    "for trial in range(5):\n",
    "    np.random.seed(42 + trial)\n",
    "    skew_trial, _ = calculate_skewness(A=A, B=B, C=C, D=D, n_simulations=3000)\n",
    "    skewness_values.append(skew_trial)\n",
    "    print(f\"Trial {trial+1}: {skew_trial:.3f}\")\n",
    "\n",
    "mean_skewness = np.mean(skewness_values)\n",
    "print(f\"\\nMean skewness across trials: {mean_skewness:.3f}\")\n",
    "\n",
    "# Final comparison\n",
    "skew_differences_mean = [abs(mean_skewness - opt) for opt in skew_options]\n",
    "closest_skew_idx_mean = np.argmin(skew_differences_mean)\n",
    "print(f\"Closest option based on mean: {skew_option_labels[closest_skew_idx_mean]} - difference: {skew_differences_mean[closest_skew_idx_mean]:.3f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "if abs(mean_skewness) < 0.1:\n",
    "    print(\"- Distribution is approximately symmetric (skewness ≈ 0)\")\n",
    "elif mean_skewness > 0:\n",
    "    print(\"- Distribution is right-skewed (positive skew)\")\n",
    "else:\n",
    "    print(\"- Distribution is left-skewed (negative skew)\")\n",
    "\n",
    "# Let's also do a quick theoretical check\n",
    "print(f\"\\nTheoretical expectation:\")\n",
    "print(f\"- OLS estimates are unbiased, so E[β̂] = β = {A}\")\n",
    "print(f\"- With large sample size and normal errors, estimates should be approximately normal\")\n",
    "print(f\"- Normal distribution has skewness = 0\")\n",
    "print(f\"- Small deviations from 0 are expected due to sampling variability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ff9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION 3: FINDING B VALUE FOR 50% POWER\n",
      "============================================================\n",
      "Testing the given options:\n",
      "Option A (1.8): B = 1.8, Power = 100.0%\n",
      "Option B (0.6): B = 0.6, Power = 46.9%\n",
      "Option C (0.2): B = 0.2, Power = 9.9%\n",
      "Option D (5.4): B = 5.4, Power = 100.0%\n",
      "\n",
      "Closest to 50% power:\n",
      "Option B (0.6) - Power: 46.9%, Difference from 50%: 0.031\n",
      "\n",
      "Verification with multiple trials for Option B (0.6):\n",
      "Trial 1: 46.5%\n",
      "Trial 2: 48.6%\n",
      "Trial 3: 48.5%\n",
      "Trial 4: 48.0%\n",
      "Trial 5: 45.4%\n",
      "Mean power across trials: 47.4%\n",
      "\n",
      "Interpretation:\n",
      "- As B increases, noise in X increases\n",
      "- More noise in X makes it harder to detect the effect of X on Y\n",
      "- Higher B → Lower power\n",
      "- We need B = 0.6 to get approximately 50% power\n",
      "\n",
      "Summary of all options:\n",
      "Option A (1.8): B = 1.8 → Power = 100.0%\n",
      "Option B (0.6): B = 0.6 → Power = 46.9%\n",
      "Option C (0.2): B = 0.2 → Power = 9.9%\n",
      "Option D (5.4): B = 5.4 → Power = 100.0%\n",
      "\n",
      "Conceptual relationship:\n",
      "- B controls noise in X: X = W + N(0,B)\n",
      "- Larger B → more noise in X → less precise estimates → lower power\n",
      "- From our results: B = 1 gave ~88% power, so we need larger B for 50% power\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate(A=1, B=1, C=10, D=1000):\n",
    "    \"\"\"\n",
    "    Simulate data according to the given model:\n",
    "    W ~ N(0,1)\n",
    "    X = W + N(0,B)\n",
    "    Y = A*X - W + N(0,C)\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, D)\n",
    "    X = W + np.random.normal(0, B, D)\n",
    "    Y = A*X - W + np.random.normal(0, C, D)\n",
    "    return Y, X, W\n",
    "\n",
    "def manual_regression_with_W(Y, X, W):\n",
    "    \"\"\"\n",
    "    Manually calculate regression Y ~ X + W and return the t-statistic for X\n",
    "    \"\"\"\n",
    "    # Create design matrix with intercept, X, and W\n",
    "    n = len(Y)\n",
    "    X_matrix = np.column_stack([np.ones(n), X, W])\n",
    "    \n",
    "    # Calculate coefficients using normal equation: β = (X'X)^(-1)X'Y\n",
    "    XTX = X_matrix.T @ X_matrix\n",
    "    XTY = X_matrix.T @ Y\n",
    "    coefficients = np.linalg.solve(XTX, XTY)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    Y_pred = X_matrix @ coefficients\n",
    "    residuals = Y - Y_pred\n",
    "    \n",
    "    # Calculate residual sum of squares\n",
    "    RSS = np.sum(residuals**2)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = n - X_matrix.shape[1]  # n - number of parameters\n",
    "    \n",
    "    # Mean squared error\n",
    "    MSE = RSS / df\n",
    "    \n",
    "    # Variance-covariance matrix of coefficients\n",
    "    var_covar_matrix = MSE * np.linalg.inv(XTX)\n",
    "    \n",
    "    # Standard error of X coefficient (index 1)\n",
    "    se_X = np.sqrt(var_covar_matrix[1, 1])\n",
    "    \n",
    "    # t-statistic for X coefficient\n",
    "    t_stat = coefficients[1] / se_X\n",
    "    \n",
    "    return t_stat, coefficients, se_X\n",
    "\n",
    "def find_power_for_B(B_value, A=1, C=10, D=1000, n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Calculate power for a specific value of B\n",
    "    \"\"\"\n",
    "    significant_count = 0\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Generate data with specific B value\n",
    "        Y, X, W = simulate(A=A, B=B_value, C=C, D=D)\n",
    "        \n",
    "        # Run regression\n",
    "        t_stat, _, _ = manual_regression_with_W(Y, X, W)\n",
    "        \n",
    "        # Check if significant (|t| > 1.96)\n",
    "        if abs(t_stat) > 1.96:\n",
    "            significant_count += 1\n",
    "    \n",
    "    power = significant_count / n_simulations\n",
    "    return power\n",
    "\n",
    "# Question 3: Find B value for 50% power\n",
    "print(\"QUESTION 3: FINDING B VALUE FOR 50% POWER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test the given options\n",
    "print(\"Testing the given options:\")\n",
    "B_options = [1.8, 0.6, 0.2, 5.4]\n",
    "B_option_labels = ['A (1.8)', 'B (0.6)', 'C (0.2)', 'D (5.4)']\n",
    "\n",
    "np.random.seed(42)\n",
    "powers_for_options = []\n",
    "\n",
    "for i, B_val in enumerate(B_options):\n",
    "    power = find_power_for_B(B_val, A=1, C=10, D=1000, n_simulations=1000)\n",
    "    powers_for_options.append(power)\n",
    "    print(f\"Option {B_option_labels[i]}: B = {B_val}, Power = {power:.1%}\")\n",
    "\n",
    "# Find closest to 50%\n",
    "target_power = 0.5\n",
    "differences_from_50 = [abs(power - target_power) for power in powers_for_options]\n",
    "closest_idx = np.argmin(differences_from_50)\n",
    "print(f\"\\nClosest to 50% power:\")\n",
    "print(f\"Option {B_option_labels[closest_idx]} - Power: {powers_for_options[closest_idx]:.1%}, Difference from 50%: {differences_from_50[closest_idx]:.3f}\")\n",
    "\n",
    "# Verify with multiple trials for the best option\n",
    "print(f\"\\nVerification with multiple trials for Option {B_option_labels[closest_idx]}:\")\n",
    "verification_powers = []\n",
    "for trial in range(5):\n",
    "    np.random.seed(42 + trial)\n",
    "    power = find_power_for_B(B_options[closest_idx], A=1, C=10, D=1000, n_simulations=800)\n",
    "    verification_powers.append(power)\n",
    "    print(f\"Trial {trial+1}: {power:.1%}\")\n",
    "\n",
    "mean_verification_power = np.mean(verification_powers)\n",
    "print(f\"Mean power across trials: {mean_verification_power:.1%}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- As B increases, noise in X increases\")\n",
    "print(f\"- More noise in X makes it harder to detect the effect of X on Y\")\n",
    "print(f\"- Higher B → Lower power\")\n",
    "print(f\"- We need B = {B_options[closest_idx]} to get approximately 50% power\")\n",
    "\n",
    "# Show all results for comparison\n",
    "print(f\"\\nSummary of all options:\")\n",
    "for i, (B_val, power) in enumerate(zip(B_options, powers_for_options)):\n",
    "    print(f\"Option {B_option_labels[i]}: B = {B_val:3.1f} → Power = {power:.1%}\")\n",
    "\n",
    "print(f\"\\nConceptual relationship:\")\n",
    "print(f\"- B controls noise in X: X = W + N(0,B)\")\n",
    "print(f\"- Larger B → more noise in X → less precise estimates → lower power\")\n",
    "print(f\"- From our results: B = 1 gave ~88% power, so we need larger B for 50% power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab33432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION 4: FINDING A VALUE FOR 50% POWER\n",
      "Parameters: B=1, C=10, D=100 (smaller sample size)\n",
      "============================================================\n",
      "Testing the given options:\n",
      "Option A (4.0): A = 4.0, Power = 97.1%\n",
      "Option B (0.5): A = 0.5, Power = 8.3%\n",
      "Option C (2.0): A = 2.0, Power = 51.3%\n",
      "Option D (1.0): A = 1.0, Power = 18.6%\n",
      "\n",
      "Closest to 50% power:\n",
      "Option C (2.0) - Power: 51.3%, Difference from 50%: 0.013\n",
      "\n",
      "Verification with multiple trials for Option C (2.0):\n",
      "Trial 1: 54.0%\n",
      "Trial 2: 52.0%\n",
      "Trial 3: 54.2%\n",
      "Trial 4: 52.8%\n",
      "Trial 5: 49.6%\n",
      "Mean power across trials: 52.5%\n",
      "\n",
      "Interpretation:\n",
      "- A is the true effect size (coefficient of X in Y = A*X - W + noise)\n",
      "- Larger A → stronger true effect → easier to detect → higher power\n",
      "- Smaller sample size (D=100) reduces power compared to D=1000\n",
      "- We need A = 2.0 to get approximately 50% power\n",
      "\n",
      "Summary of all options:\n",
      "Option A (4.0): A = 4.0 → Power = 97.1%\n",
      "Option B (0.5): A = 0.5 → Power = 8.3%\n",
      "Option C (2.0): A = 2.0 → Power = 51.3%\n",
      "Option D (1.0): A = 1.0 → Power = 18.6%\n",
      "\n",
      "Comparison with Question 1:\n",
      "- Question 1: A=1, B=1, C=10, D=1000 → Power ≈ 88%\n",
      "- Question 4: A=?, B=1, C=10, D=100 → Power ≈ 50%\n",
      "- Smaller sample size (100 vs 1000) requires larger effect size for same power\n",
      "\n",
      "Direct comparison:\n",
      "A=1, D=100:  Power = 16.6%\n",
      "A=1, D=1000: Power = 87.2%\n",
      "Sample size effect: 70.6% power increase with 10x sample size\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate(A=1, B=1, C=10, D=1000):\n",
    "    \"\"\"\n",
    "    Simulate data according to the given model:\n",
    "    W ~ N(0,1)\n",
    "    X = W + N(0,B)\n",
    "    Y = A*X - W + N(0,C)\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, D)\n",
    "    X = W + np.random.normal(0, B, D)\n",
    "    Y = A*X - W + np.random.normal(0, C, D)\n",
    "    return Y, X, W\n",
    "\n",
    "def manual_regression_with_W(Y, X, W):\n",
    "    \"\"\"\n",
    "    Manually calculate regression Y ~ X + W and return the t-statistic for X\n",
    "    \"\"\"\n",
    "    # Create design matrix with intercept, X, and W\n",
    "    n = len(Y)\n",
    "    X_matrix = np.column_stack([np.ones(n), X, W])\n",
    "    \n",
    "    # Calculate coefficients using normal equation: β = (X'X)^(-1)X'Y\n",
    "    XTX = X_matrix.T @ X_matrix\n",
    "    XTY = X_matrix.T @ Y\n",
    "    coefficients = np.linalg.solve(XTX, XTY)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    Y_pred = X_matrix @ coefficients\n",
    "    residuals = Y - Y_pred\n",
    "    \n",
    "    # Calculate residual sum of squares\n",
    "    RSS = np.sum(residuals**2)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = n - X_matrix.shape[1]  # n - number of parameters\n",
    "    \n",
    "    # Mean squared error\n",
    "    MSE = RSS / df\n",
    "    \n",
    "    # Variance-covariance matrix of coefficients\n",
    "    var_covar_matrix = MSE * np.linalg.inv(XTX)\n",
    "    \n",
    "    # Standard error of X coefficient (index 1)\n",
    "    se_X = np.sqrt(var_covar_matrix[1, 1])\n",
    "    \n",
    "    # t-statistic for X coefficient\n",
    "    t_stat = coefficients[1] / se_X\n",
    "    \n",
    "    return t_stat, coefficients, se_X\n",
    "\n",
    "def find_power_for_A(A_value, B=1, C=10, D=100, n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Calculate power for a specific value of A (effect size)\n",
    "    \"\"\"\n",
    "    significant_count = 0\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Generate data with specific A value\n",
    "        Y, X, W = simulate(A=A_value, B=B, C=C, D=D)\n",
    "        \n",
    "        # Run regression\n",
    "        t_stat, _, _ = manual_regression_with_W(Y, X, W)\n",
    "        \n",
    "        # Check if significant (|t| > 1.96)\n",
    "        if abs(t_stat) > 1.96:\n",
    "            significant_count += 1\n",
    "    \n",
    "    power = significant_count / n_simulations\n",
    "    return power\n",
    "\n",
    "# Question 4: Find A value for 50% power with smaller sample size\n",
    "print(\"QUESTION 4: FINDING A VALUE FOR 50% POWER\")\n",
    "print(\"Parameters: B=1, C=10, D=100 (smaller sample size)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test the given options\n",
    "print(\"Testing the given options:\")\n",
    "A_options = [4.0, 0.5, 2.0, 1.0]\n",
    "A_option_labels = ['A (4.0)', 'B (0.5)', 'C (2.0)', 'D (1.0)']\n",
    "\n",
    "np.random.seed(42)\n",
    "powers_for_options = []\n",
    "\n",
    "for i, A_val in enumerate(A_options):\n",
    "    power = find_power_for_A(A_val, B=1, C=10, D=100, n_simulations=1000)\n",
    "    powers_for_options.append(power)\n",
    "    print(f\"Option {A_option_labels[i]}: A = {A_val}, Power = {power:.1%}\")\n",
    "\n",
    "# Find closest to 50%\n",
    "target_power = 0.5\n",
    "differences_from_50 = [abs(power - target_power) for power in powers_for_options]\n",
    "closest_idx = np.argmin(differences_from_50)\n",
    "print(f\"\\nClosest to 50% power:\")\n",
    "print(f\"Option {A_option_labels[closest_idx]} - Power: {powers_for_options[closest_idx]:.1%}, Difference from 50%: {differences_from_50[closest_idx]:.3f}\")\n",
    "\n",
    "# Verify with multiple trials for the best option\n",
    "print(f\"\\nVerification with multiple trials for Option {A_option_labels[closest_idx]}:\")\n",
    "verification_powers = []\n",
    "for trial in range(5):\n",
    "    np.random.seed(42 + trial)\n",
    "    power = find_power_for_A(A_options[closest_idx], B=1, C=10, D=100, n_simulations=800)\n",
    "    verification_powers.append(power)\n",
    "    print(f\"Trial {trial+1}: {power:.1%}\")\n",
    "\n",
    "mean_verification_power = np.mean(verification_powers)\n",
    "print(f\"Mean power across trials: {mean_verification_power:.1%}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- A is the true effect size (coefficient of X in Y = A*X - W + noise)\")\n",
    "print(f\"- Larger A → stronger true effect → easier to detect → higher power\")\n",
    "print(f\"- Smaller sample size (D=100) reduces power compared to D=1000\")\n",
    "print(f\"- We need A = {A_options[closest_idx]} to get approximately 50% power\")\n",
    "\n",
    "# Show all results for comparison\n",
    "print(f\"\\nSummary of all options:\")\n",
    "for i, (A_val, power) in enumerate(zip(A_options, powers_for_options)):\n",
    "    print(f\"Option {A_option_labels[i]}: A = {A_val:3.1f} → Power = {power:.1%}\")\n",
    "\n",
    "print(f\"\\nComparison with Question 1:\")\n",
    "print(f\"- Question 1: A=1, B=1, C=10, D=1000 → Power ≈ 88%\")\n",
    "print(f\"- Question 4: A=?, B=1, C=10, D=100 → Power ≈ 50%\")\n",
    "print(f\"- Smaller sample size (100 vs 1000) requires larger effect size for same power\")\n",
    "\n",
    "# Let's also test A=1 with the new sample size for comparison\n",
    "print(f\"\\nDirect comparison:\")\n",
    "power_A1_D100 = find_power_for_A(1.0, B=1, C=10, D=100, n_simulations=1000)\n",
    "power_A1_D1000 = find_power_for_A(1.0, B=1, C=10, D=1000, n_simulations=1000)\n",
    "print(f\"A=1, D=100:  Power = {power_A1_D100:.1%}\")\n",
    "print(f\"A=1, D=1000: Power = {power_A1_D1000:.1%}\")\n",
    "print(f\"Sample size effect: {power_A1_D1000 - power_A1_D100:.1%} power increase with 10x sample size\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
